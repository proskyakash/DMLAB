{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2018IMT_012_DM(Program2).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVEpb2DKmG4f"
      },
      "source": [
        "\n",
        "\n",
        "*   **Name-**Akash Verma\n",
        "\n",
        "*   **Roll no.-**2018IMT-012\n",
        "*  **Course-** DM Lab\n",
        "*   **Program-2**\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "32500GW5SDGH"
      },
      "source": [
        "import pandas as pd\n",
        "import copy\n",
        "import math\n",
        "import numpy as np\n",
        "from numpy import var, mean\n",
        "import operator as op\n",
        "from functools import reduce\n",
        "from random import shuffle, seed\n",
        "from collections import defaultdict\n",
        "import warnings\n",
        "from copy import deepcopy\n",
        "from abc import ABC, abstractmethod"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "VJzHsmwoSajt",
        "outputId": "8ee6ea79-9067-4fc8-bb25-3adefdc09810"
      },
      "source": [
        "dataSet = pd.read_csv('dmml dataset.csv')\n",
        "dataSet.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Unnamed: 0</th>\n",
              "      <th>Category</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>ALB</th>\n",
              "      <th>ALP</th>\n",
              "      <th>ALT</th>\n",
              "      <th>AST</th>\n",
              "      <th>BIL</th>\n",
              "      <th>CHE</th>\n",
              "      <th>CHOL</th>\n",
              "      <th>CREA</th>\n",
              "      <th>GGT</th>\n",
              "      <th>PROT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>0=Blood Donor</td>\n",
              "      <td>32</td>\n",
              "      <td>m</td>\n",
              "      <td>38.5</td>\n",
              "      <td>52.5</td>\n",
              "      <td>7.7</td>\n",
              "      <td>22.1</td>\n",
              "      <td>7.5</td>\n",
              "      <td>6.93</td>\n",
              "      <td>3.23</td>\n",
              "      <td>106.0</td>\n",
              "      <td>12.1</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Unnamed: 0       Category  Age Sex   ALB  ...   CHE  CHOL   CREA   GGT  PROT\n",
              "0           1  0=Blood Donor   32   m  38.5  ...  6.93  3.23  106.0  12.1  69.0\n",
              "\n",
              "[1 rows x 14 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Upxl7If1Tfg3"
      },
      "source": [
        "dataSet.drop('Unnamed: 0',axis='columns',inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9FKflQpTTpQF"
      },
      "source": [
        "dataSet.dropna(inplace=True)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zQ8j3lBiTwUN"
      },
      "source": [
        "df=dataSet.Sex.map({'m':0,'f':1})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwt6ZdvNT3Rd",
        "outputId": "bd3eac45-8097-422c-8c23-1e7fd517b796"
      },
      "source": [
        "df.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iflPAi7zT4l2",
        "outputId": "a77536c7-7cdd-4a20-ea29-66867ba6f174"
      },
      "source": [
        "dataSet.isnull().sum()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Category    0\n",
              "Age         0\n",
              "Sex         0\n",
              "ALB         0\n",
              "ALP         0\n",
              "ALT         0\n",
              "AST         0\n",
              "BIL         0\n",
              "CHE         0\n",
              "CHOL        0\n",
              "CREA        0\n",
              "GGT         0\n",
              "PROT        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6Rb0-Wp-T_nt",
        "outputId": "691653c4-2333-4abb-bf0a-f11819ad2d15"
      },
      "source": [
        "dataSet.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 589 entries, 0 to 612\n",
            "Data columns (total 13 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Category  589 non-null    object \n",
            " 1   Age       589 non-null    int64  \n",
            " 2   Sex       589 non-null    object \n",
            " 3   ALB       589 non-null    float64\n",
            " 4   ALP       589 non-null    float64\n",
            " 5   ALT       589 non-null    float64\n",
            " 6   AST       589 non-null    float64\n",
            " 7   BIL       589 non-null    float64\n",
            " 8   CHE       589 non-null    float64\n",
            " 9   CHOL      589 non-null    float64\n",
            " 10  CREA      589 non-null    float64\n",
            " 11  GGT       589 non-null    float64\n",
            " 12  PROT      589 non-null    float64\n",
            "dtypes: float64(10), int64(1), object(2)\n",
            "memory usage: 64.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ui5K7WXBUDxF",
        "outputId": "b5295999-bfd3-429c-d625-4302b3abfefe"
      },
      "source": [
        "dataSet.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Category    0\n",
              "Age         0\n",
              "Sex         0\n",
              "ALB         0\n",
              "ALP         0\n",
              "ALT         0\n",
              "AST         0\n",
              "BIL         0\n",
              "CHE         0\n",
              "CHOL        0\n",
              "CREA        0\n",
              "GGT         0\n",
              "PROT        0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zo998adUUJN1",
        "outputId": "0c8b7084-cc7a-42ef-af56-db7bff55d59d"
      },
      "source": [
        "dataSet.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 589 entries, 0 to 612\n",
            "Data columns (total 13 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Category  589 non-null    object \n",
            " 1   Age       589 non-null    int64  \n",
            " 2   Sex       589 non-null    object \n",
            " 3   ALB       589 non-null    float64\n",
            " 4   ALP       589 non-null    float64\n",
            " 5   ALT       589 non-null    float64\n",
            " 6   AST       589 non-null    float64\n",
            " 7   BIL       589 non-null    float64\n",
            " 8   CHE       589 non-null    float64\n",
            " 9   CHOL      589 non-null    float64\n",
            " 10  CREA      589 non-null    float64\n",
            " 11  GGT       589 non-null    float64\n",
            " 12  PROT      589 non-null    float64\n",
            "dtypes: float64(10), int64(1), object(2)\n",
            "memory usage: 64.4+ KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-aPBuyYRUNJO",
        "outputId": "ecf28194-8d24-4405-b762-b2271db6e17a"
      },
      "source": [
        "dataSet.Category.unique()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array(['0=Blood Donor', '0s=suspect Blood Donor', '1=Hepatitis',\n",
              "       '2=Fibrosis', '3=Cirrhosis'], dtype=object)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xIAIQ6HGUPmO",
        "outputId": "466866aa-713b-479b-cfbb-ed62cc7f0ec0"
      },
      "source": [
        "df1=dataSet.Category.map({'0=Blood Donor':1,'0s=suspect Blood Donor':2,'1=Hepatitis':3,'2=Fibrosis':4,'3=Cirrhosis':5})\n",
        "df1.count()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "589"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "id": "Oc8xQRzUUUhu",
        "outputId": "1c72db48-6056-4f56-c9af-a3ba1f4877e4"
      },
      "source": [
        "dataSet.head()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Category</th>\n",
              "      <th>Age</th>\n",
              "      <th>Sex</th>\n",
              "      <th>ALB</th>\n",
              "      <th>ALP</th>\n",
              "      <th>ALT</th>\n",
              "      <th>AST</th>\n",
              "      <th>BIL</th>\n",
              "      <th>CHE</th>\n",
              "      <th>CHOL</th>\n",
              "      <th>CREA</th>\n",
              "      <th>GGT</th>\n",
              "      <th>PROT</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0=Blood Donor</td>\n",
              "      <td>32</td>\n",
              "      <td>m</td>\n",
              "      <td>38.5</td>\n",
              "      <td>52.5</td>\n",
              "      <td>7.7</td>\n",
              "      <td>22.1</td>\n",
              "      <td>7.5</td>\n",
              "      <td>6.93</td>\n",
              "      <td>3.23</td>\n",
              "      <td>106.0</td>\n",
              "      <td>12.1</td>\n",
              "      <td>69.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0=Blood Donor</td>\n",
              "      <td>32</td>\n",
              "      <td>m</td>\n",
              "      <td>38.5</td>\n",
              "      <td>70.3</td>\n",
              "      <td>18.0</td>\n",
              "      <td>24.7</td>\n",
              "      <td>3.9</td>\n",
              "      <td>11.17</td>\n",
              "      <td>4.80</td>\n",
              "      <td>74.0</td>\n",
              "      <td>15.6</td>\n",
              "      <td>76.5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0=Blood Donor</td>\n",
              "      <td>32</td>\n",
              "      <td>m</td>\n",
              "      <td>46.9</td>\n",
              "      <td>74.7</td>\n",
              "      <td>36.2</td>\n",
              "      <td>52.6</td>\n",
              "      <td>6.1</td>\n",
              "      <td>8.84</td>\n",
              "      <td>5.20</td>\n",
              "      <td>86.0</td>\n",
              "      <td>33.2</td>\n",
              "      <td>79.3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0=Blood Donor</td>\n",
              "      <td>32</td>\n",
              "      <td>m</td>\n",
              "      <td>43.2</td>\n",
              "      <td>52.0</td>\n",
              "      <td>30.6</td>\n",
              "      <td>22.6</td>\n",
              "      <td>18.9</td>\n",
              "      <td>7.33</td>\n",
              "      <td>4.74</td>\n",
              "      <td>80.0</td>\n",
              "      <td>33.8</td>\n",
              "      <td>75.7</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0=Blood Donor</td>\n",
              "      <td>32</td>\n",
              "      <td>m</td>\n",
              "      <td>39.2</td>\n",
              "      <td>74.1</td>\n",
              "      <td>32.6</td>\n",
              "      <td>24.8</td>\n",
              "      <td>9.6</td>\n",
              "      <td>9.15</td>\n",
              "      <td>4.32</td>\n",
              "      <td>76.0</td>\n",
              "      <td>29.9</td>\n",
              "      <td>68.7</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        Category  Age Sex   ALB   ALP  ...    CHE  CHOL   CREA   GGT  PROT\n",
              "0  0=Blood Donor   32   m  38.5  52.5  ...   6.93  3.23  106.0  12.1  69.0\n",
              "1  0=Blood Donor   32   m  38.5  70.3  ...  11.17  4.80   74.0  15.6  76.5\n",
              "2  0=Blood Donor   32   m  46.9  74.7  ...   8.84  5.20   86.0  33.2  79.3\n",
              "3  0=Blood Donor   32   m  43.2  52.0  ...   7.33  4.74   80.0  33.8  75.7\n",
              "4  0=Blood Donor   32   m  39.2  74.1  ...   9.15  4.32   76.0  29.9  68.7\n",
              "\n",
              "[5 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6dQKQ6NXUYtd"
      },
      "source": [
        "dataSet.drop('Sex',axis='columns',inplace=True)\n",
        "dataSet.drop('Category',axis='columns',inplace=True)\n",
        "dataSet=pd.concat([df,dataSet,df1],axis=1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 77
        },
        "id": "Y3SJyynMx5Ly",
        "outputId": "40a27cad-40a8-4ee6-c5b3-9a46dcc008c7"
      },
      "source": [
        "dataSet.head(1)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Sex</th>\n",
              "      <th>Age</th>\n",
              "      <th>ALB</th>\n",
              "      <th>ALP</th>\n",
              "      <th>ALT</th>\n",
              "      <th>AST</th>\n",
              "      <th>BIL</th>\n",
              "      <th>CHE</th>\n",
              "      <th>CHOL</th>\n",
              "      <th>CREA</th>\n",
              "      <th>GGT</th>\n",
              "      <th>PROT</th>\n",
              "      <th>Category</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>32</td>\n",
              "      <td>38.5</td>\n",
              "      <td>52.5</td>\n",
              "      <td>7.7</td>\n",
              "      <td>22.1</td>\n",
              "      <td>7.5</td>\n",
              "      <td>6.93</td>\n",
              "      <td>3.23</td>\n",
              "      <td>106.0</td>\n",
              "      <td>12.1</td>\n",
              "      <td>69.0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   Sex  Age   ALB   ALP  ALT   AST  ...   CHE  CHOL   CREA   GGT  PROT  Category\n",
              "0    0   32  38.5  52.5  7.7  22.1  ...  6.93  3.23  106.0  12.1  69.0         1\n",
              "\n",
              "[1 rows x 13 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Bk_CwYZjTCTN",
        "outputId": "1b42fb1c-485d-4bf3-e0ce-3457df8a19c3"
      },
      "source": [
        "dataSet.info()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "Int64Index: 589 entries, 0 to 612\n",
            "Data columns (total 13 columns):\n",
            " #   Column    Non-Null Count  Dtype  \n",
            "---  ------    --------------  -----  \n",
            " 0   Sex       589 non-null    int64  \n",
            " 1   Age       589 non-null    int64  \n",
            " 2   ALB       589 non-null    float64\n",
            " 3   ALP       589 non-null    float64\n",
            " 4   ALT       589 non-null    float64\n",
            " 5   AST       589 non-null    float64\n",
            " 6   BIL       589 non-null    float64\n",
            " 7   CHE       589 non-null    float64\n",
            " 8   CHOL      589 non-null    float64\n",
            " 9   CREA      589 non-null    float64\n",
            " 10  GGT       589 non-null    float64\n",
            " 11  PROT      589 non-null    float64\n",
            " 12  Category  589 non-null    int64  \n",
            "dtypes: float64(10), int64(3)\n",
            "memory usage: 64.4 KB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_mnnvN0MT1td",
        "outputId": "c621ff7a-25e8-4eb0-9e46-96b8d73f97f6"
      },
      "source": [
        "dataSet.isnull().sum()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Sex         0\n",
              "Age         0\n",
              "ALB         0\n",
              "ALP         0\n",
              "ALT         0\n",
              "AST         0\n",
              "BIL         0\n",
              "CHE         0\n",
              "CHOL        0\n",
              "CREA        0\n",
              "GGT         0\n",
              "PROT        0\n",
              "Category    0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UGSmeKpUUyE-"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "train, test = train_test_split(dataSet,test_size=0.3,random_state=40)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pwDJGkJqWEpB"
      },
      "source": [
        "#base.py\n",
        "class Ruleset:\n",
        "    \"\"\"Collection of Rules in disjunctive normal form.\"\"\"\n",
        "\n",
        "    def __init__(self, rules=None):\n",
        "        if rules is None:\n",
        "            self.rules = []\n",
        "        else:\n",
        "            self.rules = rules\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"[\" + \" V \".join([str(rule) for rule in self.rules]) + \"]\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        ruleset_str = self.__str__()\n",
        "        return f\"<Ruleset {ruleset_str}>\"\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        return self.rules[index]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rules)\n",
        "\n",
        "    def truncstr(self, limit=2, direction=\"left\"):\n",
        "        \"\"\"Return Ruleset string representation.\n",
        "        limit : int, default=2\n",
        "            Maximum number of rules to include in string.\n",
        "        Direction : str, default=\"left\"\n",
        "            Which end of ruleset to return. Valid options: 'left', 'right'.\n",
        "        \"\"\"\n",
        "        if len(self.rules) > limit:\n",
        "            if direction == \"left\":\n",
        "                return Ruleset(self.rules[:limit]).__str__() + \"...\"\n",
        "            elif direction == \"right\":\n",
        "                return \"...\" + Ruleset(self.rules[-limit:]).__str__()\n",
        "            else:\n",
        "                raise ValueError('direction param must be \"left\" or \"right\"')\n",
        "        else:\n",
        "            return self.__str__()\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if type(other) != Ruleset:\n",
        "            return False\n",
        "\n",
        "        for r in self.rules:\n",
        "            # TODO: Ideally, should implement a hash function--in practice speedup would be insignificant\n",
        "            if r not in other.rules:\n",
        "                return False\n",
        "        for (\n",
        "            r\n",
        "        ) in (\n",
        "            other.rules\n",
        "        ):  # Check the other way around too. (Can't compare lengths instead b/c there might be duplicate rules.)\n",
        "            if r not in self.rules:\n",
        "                return False\n",
        "        return True\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.rules)\n",
        "\n",
        "    def out_pretty(self):\n",
        "        \"\"\"Print Ruleset line-by-line.\"\"\"\n",
        "        ruleset_str = (\n",
        "            str([str(rule) for rule in self.rules])\n",
        "            .replace(\" \", \"\")\n",
        "            .replace(\",\", \" V\\n\")\n",
        "            .replace(\"'\", \"\")\n",
        "            .replace(\"^\", \" ^ \")\n",
        "        )\n",
        "        print(ruleset_str)\n",
        "\n",
        "    def isuniversal(self):\n",
        "        \"\"\"Return whether the Ruleset has an empty rule, i.e. it will always return positive predictions.\"\"\"\n",
        "        if len(self.rules) >= 1:\n",
        "            return all(rule.isempty() for rule in self.rules)\n",
        "        else:\n",
        "            return False\n",
        "\n",
        "    def isnull(self):\n",
        "        \"\"\"Return whether the Ruleset has no rules, i.e. it will always return negative predictions.\"\"\"\n",
        "        return len(self.rules) == 0\n",
        "\n",
        "    def copy(self, n_rules_limit=None):\n",
        "        \"\"\"Return a deep copy of ruleset.\n",
        "        n_rules_limit : default=None\n",
        "            Limit copy to this a subset of original rules.\n",
        "        \"\"\"\n",
        "        result = copy.deepcopy(self)\n",
        "        if n_rules_limit is not None:\n",
        "            result.rules = result.rules[:n_rules_limit]\n",
        "        return result\n",
        "\n",
        "    def covers(self, df):\n",
        "        \"\"\"Return covered examples.\"\"\"\n",
        "\n",
        "        self._check_allpos_allneg(warn=False)\n",
        "        if self.isuniversal():\n",
        "            return df\n",
        "        elif self.isnull():\n",
        "            return df.head(0)\n",
        "        else:\n",
        "            covered = self.rules[0].covers(df).copy()\n",
        "            for rule in self.rules[1:]:\n",
        "                covered = covered.append(rule.covers(df))\n",
        "            covered = covered.drop_duplicates()\n",
        "            return covered\n",
        "\n",
        "    def num_covered(self, df):\n",
        "        \"\"\"Return the number of covered examples.\"\"\"\n",
        "        return len(self.covers(df))\n",
        "\n",
        "    def count_rules(self):\n",
        "        \"\"\"Return number of rules in the Ruleset.\"\"\"\n",
        "        return len(self.rules)\n",
        "\n",
        "    def count_conds(self):\n",
        "        \"\"\"Return total number of conds in the Ruleset.\"\"\"\n",
        "        return sum([len(r.conds) for r in self.rules])\n",
        "\n",
        "    def get_conds(self):\n",
        "        \"\"\"Return list of all selected conds\"\"\"\n",
        "        conds_list = []\n",
        "        conds_set = set()\n",
        "        for r in self.rules:\n",
        "            new_conds = [c for c in r.conds if c not in conds_set]\n",
        "            conds_list.extend(new_conds)\n",
        "            conds_set.update(set(new_conds))\n",
        "        return conds_list\n",
        "\n",
        "    def _set_possible_conds(self, pos_df, neg_df):\n",
        "        \"\"\" Stores a list of all possible conds. \"\"\"\n",
        "\n",
        "        # Used in Rule::successors so as not to rebuild it each time,\n",
        "        # and in exceptions_dl calculations because nCr portion of formula already accounts for no replacement.)\n",
        "\n",
        "        self.possible_conds = []\n",
        "        for feat in pos_df.columns.values:\n",
        "            for val in set(pos_df[feat].unique()).intersection(\n",
        "                set(neg_df[feat].unique())\n",
        "            ):\n",
        "                self.possible_conds.append(Cond(feat, val))\n",
        "\n",
        "    def trim_conds(self, max_total_conds=None):\n",
        "        \"\"\".Reduce the total number of Conds in a Ruleset by removing Rules.\"\"\"\n",
        "        if max_total_conds is not None:\n",
        "            while len(self.rules) > 0 and self.count_conds() > max_total_conds:\n",
        "                self.rules.pop(-1)\n",
        "\n",
        "    def trimmed_str(iterable, max_items=3):\n",
        "        return str(iterable[:max_items])[-1] + \"...\"\n",
        "\n",
        "    def add(self, rule):\n",
        "        \"\"\"Add a rule.\"\"\"\n",
        "        self.rules.append(asrule(rule))\n",
        "\n",
        "    def remove(self, index):\n",
        "        _check_valid_index(index, self, \"remove\")\n",
        "        del self.rules[index]\n",
        "\n",
        "    def remove_rule(self, old_rule):\n",
        "        _check_rule_exists(asrule(old_rule), self, \"remove_rule\")\n",
        "        index = self.rules.index(asrule(old_rule))\n",
        "        self.remove(index)\n",
        "\n",
        "    def insert(self, index, new_rule):\n",
        "        _check_valid_index(index, self, \"insert\")\n",
        "        self.rules.insert(index, asrule(new_rule))\n",
        "\n",
        "    def insert_rule(self, insert_before_rule, new_rule):\n",
        "        _check_rule_exists(asrule(insert_before_rule), self, \"replace_rule\")\n",
        "        index = self.rules.index(asrule(insert_before_rule))\n",
        "        self.insert(index, asrule(new_rule))\n",
        "\n",
        "    def replace(self, index, new_rule):\n",
        "        _check_valid_index(index, self, \"replace\")\n",
        "        self.rules[index] = asrule(new_rule)\n",
        "\n",
        "    def replace_rule(self, old_rule, new_rule):\n",
        "        _check_rule_exists(asrule(old_rule), self, \"replace_rule\")\n",
        "        index = self.rules.index(asrule(old_rule))\n",
        "        self.replace(index, asrule(new_rule))\n",
        "\n",
        "    def predict(self, X_df, give_reasons=False, warn=True):\n",
        "        # Issue warning if Ruleset is universal or empty\n",
        "        self._check_allpos_allneg(warn=warn, warnstack=[(\"base\", \"predict\")])\n",
        "\n",
        "        covered_indices = set(self.covers(X_df).index.tolist())\n",
        "        predictions = [i in covered_indices for i in X_df.index]\n",
        "\n",
        "        if not give_reasons:\n",
        "            return predictions\n",
        "        else:\n",
        "            reasons = []\n",
        "            # For each Ruleset-covered example, collect list of every Rule that covers it;\n",
        "            # for non-covered examples, collect an empty list\n",
        "            for i, p in zip(X_df.index, predictions):\n",
        "                example = X_df[X_df.index == i]\n",
        "                example_reasons = (\n",
        "                    [rule for rule in self.rules if len(rule.covers(example)) == 1]\n",
        "                    if p\n",
        "                    else []\n",
        "                )\n",
        "                reasons.append(example_reasons)\n",
        "            return (predictions, reasons)\n",
        "\n",
        "    def predict_proba(self, X_df, give_reasons=False):\n",
        "      \n",
        "\n",
        "        # Get proba for all negative predictions\n",
        "        uncovered_proba = weighted_avg_freqs([self.uncovered_class_freqs])\n",
        "\n",
        "        # Make predictions for each example\n",
        "        predictions, covering_rules = self.predict(X_df, give_reasons=True, warn=False)\n",
        "\n",
        "        # Calculate probas for each example\n",
        "        invalid_example_idx = []\n",
        "        probas = np.empty(shape=(len(predictions), uncovered_proba.shape[0]))\n",
        "        for i, (p, cr) in enumerate(zip(predictions, covering_rules)):\n",
        "            if not p:\n",
        "                probas[i, :] = uncovered_proba\n",
        "            else:\n",
        "                # Make sure only using rules that had enough samples to record\n",
        "                valid_class_freqs = [\n",
        "                    rule.class_freqs for rule in cr if rule.class_freqs is not None\n",
        "                ]\n",
        "                if valid_class_freqs:\n",
        "                    probas[i, :] = weighted_avg_freqs(valid_class_freqs)\n",
        "                else:\n",
        "                    probas[i, :] = 0\n",
        "                    invalid_example_idx.append(i)\n",
        "\n",
        "        # Warn if any examples didn't have large enough sample size of any rules\n",
        "        if invalid_example_idx:\n",
        "            warning_str = f\"Some examples lacked any rule with sufficient sample size to predict_proba: {invalid_example_idx}\\n Consider running recalibrate_proba with smaller param min_samples, or set require_min_samples=False\"\n",
        "            _warn(\n",
        "                warning_str, RuntimeWarning, filename=\"base\", funcname=\"predict_proba\",\n",
        "            )\n",
        "        # return probas (and optional extras)\n",
        "        result = flagged_return([True, give_reasons], [probas, covering_rules])\n",
        "        return result\n",
        "\n",
        "    def _check_allpos_allneg(self, warn=False, warnstack=\"\"):\n",
        "        \"\"\"Check if a Ruleset is universal (always predicts pos) or empty (always predicts neg) \"\"\"\n",
        "        if self.isuniversal() and warn:\n",
        "            warning_str = f\"Ruleset is universal. All predictions it makes with method .predict will be positive. It may be untrained or was trained on a dataset split lacking negative examples.\"\n",
        "            _warn(\n",
        "                warning_str,\n",
        "                RuntimeWarning,\n",
        "                filename=\"base\",\n",
        "                funcname=\"_check_allpos_allneg\",\n",
        "                warnstack=warnstack,\n",
        "            )\n",
        "        elif self.isnull() and warn:\n",
        "            warning_str = f\"Ruleset is empty. All predictions it makes with method .predict will be negative. It may be untrained or was trained on a dataset split lacking positive examples.\"\n",
        "            _warn(\n",
        "                warning_str,\n",
        "                RuntimeWarning,\n",
        "                filename=\"base\",\n",
        "                funcname=\"_check_allpos_allneg\",\n",
        "                warnstack=warnstack,\n",
        "            )\n",
        "        return self.isuniversal(), self.isnull()\n",
        "\n",
        "    def get_selected_features(self):\n",
        "        \"\"\"Return list of selected features in order they were added.\"\"\"\n",
        "        feature_list = []\n",
        "        feature_set = set()\n",
        "        for rule in self.rules:\n",
        "            for cond in rule.conds:\n",
        "                feature = cond.feature\n",
        "                if feature not in feature_set:\n",
        "                    feature_list.append(feature)\n",
        "                    feature_set.add(feature)\n",
        "        return feature_list\n",
        "\n",
        "\n",
        "class Rule:\n",
        "    \"\"\"Conjunction of Conds\"\"\"\n",
        "\n",
        "    def __init__(self, conds=None):\n",
        "        if conds is None:\n",
        "            self.conds = []\n",
        "        else:\n",
        "            self.conds = conds\n",
        "\n",
        "    def __str__(self):\n",
        "        if not self.conds:\n",
        "            rule_str = \"[True]\"\n",
        "        else:\n",
        "            rule_str = (\n",
        "                str([str(cond) for cond in self.conds])\n",
        "                .replace(\",\", \"^\")\n",
        "                .replace(\"'\", \"\")\n",
        "                .replace(\" \", \"\")\n",
        "            )\n",
        "        return rule_str\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<Rule {str(self)}>\"\n",
        "\n",
        "    def __add__(self, cond):\n",
        "        if isinstance(cond, Cond):\n",
        "            return Rule(self.conds + [cond])\n",
        "        else:\n",
        "            raise TypeError(\n",
        "                f\"{self} + {cond}: Rule objects can only conjoin Cond objects.\"\n",
        "            )\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if type(other) != Rule:\n",
        "            return False\n",
        "        elif len(self.conds) != len(other.conds):\n",
        "            return False\n",
        "        return set([str(cond) for cond in self.conds]) == set(\n",
        "            [str(cond) for cond in other.conds]\n",
        "        )\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash(str([self.conds]))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.conds)\n",
        "\n",
        "    def isempty(self):\n",
        "        return len(self.conds) == 0\n",
        "\n",
        "    def covers(self, df):\n",
        "        \"\"\"Return instances covered by the Rule.\"\"\"\n",
        "        covered = df.head(len(df))\n",
        "        for cond in self.conds:\n",
        "            covered = cond.covers(covered)\n",
        "        return covered\n",
        "\n",
        "    def num_covered(self, df):\n",
        "        return len(self.covers(df))\n",
        "\n",
        "    def covered_feats(self):\n",
        "        \"\"\"Return list of features covered by the Rule.\"\"\"\n",
        "        return [cond.feature for cond in self.conds]\n",
        "\n",
        "    #############################################\n",
        "    ##### Rule::grow/prune helper functions #####\n",
        "    #############################################\n",
        "\n",
        "    def successors(self, possible_conds, pos_df, neg_df):\n",
        "       \n",
        "\n",
        "        if possible_conds is not None:\n",
        "            successor_conds = [\n",
        "                cond for cond in possible_conds if cond not in self.conds\n",
        "            ]\n",
        "            return [Rule(self.conds + [cond]) for cond in successor_conds]\n",
        "        else:\n",
        "            successor_rules = []\n",
        "            for feat in pos_df.columns.values:\n",
        "                for val in set(pos_df[feat].unique()).intersection(\n",
        "                    set(neg_df[feat].unique())\n",
        "                ):\n",
        "                    if (\n",
        "                        feat not in self.covered_feats()\n",
        "                    ):  # Conds already in Rule and Conds that contradict Rule aren't valid successors / NB Rules are short; this is unlikely to be worth the overhead of cheacking\n",
        "                        successor_rules.append(self + Cond(feat, val))\n",
        "            return successor_rules\n",
        "\n",
        "\n",
        "class Cond:\n",
        "    \"\"\"Conditional\"\"\"\n",
        "\n",
        "    def __init__(self, feature, val):\n",
        "        self.feature = feature\n",
        "        self.val = val\n",
        "\n",
        "    def __str__(self):\n",
        "        return f\"{self.feature}={self.val}\"\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f\"<Cond {self.feature}={self.val}>\"\n",
        "\n",
        "    def __eq__(self, other):\n",
        "        if type(other) != Cond:\n",
        "            return False\n",
        "        else:\n",
        "            return self.feature == other.feature and self.val == other.val\n",
        "\n",
        "    def __hash__(self):\n",
        "        return hash((self.feature, self.val))\n",
        "\n",
        "    def covers(self, df):\n",
        "        \"\"\"Return instances covered by the Cond, i.e. those which are not in contradiction with it.\"\"\"\n",
        "        return df[df[self.feature] == self.val]\n",
        "\n",
        "    def num_covered(self, df):\n",
        "        return len(self.covers(df))\n",
        "\n",
        "\n",
        "def cond_fromstr(str_):\n",
        "    antecedent_consequent = tuple(s.strip() for s in str_.split(\"=\"))\n",
        "    if len(antecedent_consequent) != 2:\n",
        "        raise ValueError(\n",
        "            f\"cond_stromstr: There was a problem with parsing the string: {str_} Cond strings should take the form of 'antecedent=consequent'. Check to ensure you're using correct logical syntax.\"\n",
        "        )\n",
        "\n",
        "    return Cond(antecedent_consequent[0], antecedent_consequent[1])\n",
        "\n",
        "\n",
        "def rule_fromstr(str_):\n",
        "    if not str or str_ == \"True\" or str_ == \"[True]\" or str_ == \"[]\":\n",
        "        return Rule()\n",
        "\n",
        "    rule_str = drop_chars(str_, \"[]\")\n",
        "    try:\n",
        "        conds = [cond_fromstr(s) for s in rule_str.split(\"^\")]\n",
        "    except:\n",
        "        raise ValueError(\n",
        "            f\"rule_stromstr: There was a problem with parsing the string: '{str_}' Rule strings should take the form of '[antecedent1=consequent1 ^ antecedent2=consequent2...]' Check to ensure you're using correct logical syntax.\"\n",
        "        )\n",
        "    return Rule(conds)\n",
        "\n",
        "\n",
        "def ruleset_fromstr(str_):\n",
        "    if not str_ or str_ == \"[]\" or str_ == \"[[]]\":\n",
        "        return Ruleset()\n",
        "\n",
        "    rules = []\n",
        "    for rulestr in str_.split(\"V\"):\n",
        "        try:\n",
        "            rules.append(rule_fromstr(rulestr))\n",
        "        except:\n",
        "            raise ValueError(\n",
        "                f\"ruleset_stromstr: There was a problem with parsing the string: '{str_}' somewhere near the area of '{rulestr}' Ruleset strings should take the form of '[[antecedent1=consequent1] V [antecedent2=consequent2 ^ antecedent3=consequent3]...]' Check to ensure you're using correct logical syntax.\"\n",
        "            )\n",
        "\n",
        "    return Ruleset(rules)\n",
        "\n",
        "\n",
        "def ascond(obj):\n",
        "    if type(obj) == Cond:\n",
        "        return obj\n",
        "    elif type(obj) == str:\n",
        "        return cond_fromstr(obj)\n",
        "    elif (\n",
        "        hasattr(obj, \"__iter__\")\n",
        "        and len(obj) == 2\n",
        "        and all([type(item) == str for item in obj])\n",
        "    ):\n",
        "        return Cond(obj[0], obj[1])\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f\"ascond: {obj} type {type(obj)} cannot be converted to cond. Type should be Cond or str\"\n",
        "        )\n",
        "\n",
        "\n",
        "def asrule(obj):\n",
        "    if type(obj) == Rule:\n",
        "        return obj\n",
        "    elif type(obj) == str:\n",
        "        return rule_fromstr(obj)\n",
        "    elif hasattr(obj, \"__iter__\"):\n",
        "        try:\n",
        "            return Rule([ascond(item) for item in obj])\n",
        "        except:\n",
        "            raise TypeError(\n",
        "                f\"asrule: {obj} type {type(obj)} cannot be converted to rule. Type should be Rule, list of Conds, or str\"\n",
        "            )\n",
        "\n",
        "\n",
        "def asruleset(obj):\n",
        "    if type(obj) == Ruleset:\n",
        "        return obj\n",
        "    elif type(obj) == str:\n",
        "        return ruleset_fromstr(obj)\n",
        "    elif hasattr(obj, \"__iter__\"):\n",
        "        try:\n",
        "            return Ruleset([asrule(item) for item in obj])\n",
        "        except:\n",
        "            raise TypeError(\n",
        "                f\"asruleset: {obj} type {type(obj)} cannot be converted to rule. Type should be Ruleset, list of Rule, or str\"\n",
        "            )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kvKqYnuCWUQT"
      },
      "source": [
        "#base_functions\n",
        "\n",
        "\n",
        "\n",
        "##########################\n",
        "##### BASE FUNCTIONS #####\n",
        "##########################\n",
        "\n",
        "\n",
        "def grow_rule(\n",
        "    pos_df,\n",
        "    neg_df,\n",
        "    possible_conds,\n",
        "    initial_rule=Rule(),\n",
        "    max_rule_conds=None,\n",
        "    verbosity=0,\n",
        "):\n",
        "    \"\"\"Fit a new rule to add to a ruleset\"\"\"\n",
        "\n",
        "    rule0 = copy.deepcopy(initial_rule)\n",
        "    if verbosity >= 4:\n",
        "        print(f\"growing rule from initial rule: {rule0}\")\n",
        "\n",
        "    rule1 = copy.deepcopy(rule0)\n",
        "    while (len(rule0.covers(neg_df)) > 0 and rule1 is not None) and (\n",
        "        max_rule_conds is None or len(rule1.conds) < max_rule_conds\n",
        "    ):  # Stop refining rule if no negative examples remain\n",
        "        rule1 = best_successor(\n",
        "            rule0, possible_conds, pos_df, neg_df, verbosity=verbosity\n",
        "        )\n",
        "        if rule1 is not None:\n",
        "            rule0 = rule1\n",
        "            if verbosity >= 4:\n",
        "                print(f\"negs remaining {len(rule0.covers(neg_df))}\")\n",
        "\n",
        "    if not rule0.isempty():\n",
        "        if verbosity >= 2:\n",
        "            print(f\"grew rule: {rule0}\")\n",
        "        return rule0\n",
        "    else:\n",
        "        # warning_str = f\"grew an empty rule: {rule0} over {len(pos_idx)} pos and {len(neg_idx)} neg\"\n",
        "        # _warn(warning_str, RuntimeWarning, filename='base_functions', funcname='grow_rule')\n",
        "        return rule0\n",
        "\n",
        "\n",
        "def grow_rule_cn(\n",
        "    cn, pos_idx, neg_idx, initial_rule=Rule(), max_rule_conds=None, verbosity=0\n",
        "):\n",
        "    \"\"\"Fit a new rule to add to a ruleset. (Optimized version.)\"\"\"\n",
        "\n",
        "    rule0 = copy.deepcopy(initial_rule)\n",
        "    rule1 = copy.deepcopy(rule0)\n",
        "    if verbosity >= 4:\n",
        "        print(f\"growing rule from initial rule: {rule0}\")\n",
        "\n",
        "    num_neg_covered = len(cn.rule_covers(rule0, subset=neg_idx))\n",
        "    while num_neg_covered > 0:  # Stop refining rule if no negative examples remain\n",
        "        user_halt = max_rule_conds is not None and len(rule1.conds) >= max_rule_conds\n",
        "        if user_halt:\n",
        "            break\n",
        "\n",
        "        rule1 = best_rule_successor_cn(cn, rule0, pos_idx, neg_idx, verbosity=verbosity)\n",
        "        if rule1 is None:\n",
        "            break\n",
        "        rule0 = rule1\n",
        "        num_neg_covered = len(cn.rule_covers(rule0, neg_idx))\n",
        "        if verbosity >= 4:\n",
        "            print(f\"negs remaining: {num_neg_covered}\")\n",
        "\n",
        "    if not rule0.isempty():\n",
        "        if verbosity >= 2:\n",
        "            print(f\"grew rule: {rule0}\")\n",
        "        return rule0\n",
        "    else:\n",
        "        # warning_str = f\"grew an empty rule: {rule0} over {len(pos_idx)} pos and {len(neg_idx)} neg\"\n",
        "        # _warn(warning_str, RuntimeWarning, filename='base_functions', funcname='grow_rule_cn')\n",
        "        return rule0\n",
        "\n",
        "\n",
        "def prune_rule(\n",
        "    rule,\n",
        "    prune_metric,\n",
        "    pos_pruneset,\n",
        "    neg_pruneset,\n",
        "    eval_index_on_ruleset=None,\n",
        "    verbosity=0,\n",
        "):\n",
        " \n",
        "    if rule.isempty():\n",
        "        # warning_str = f\"can't prune empty rule: {rule}\"\n",
        "        # _warn(warning_str, RuntimeWarning, filename='base_functions', funcname='prune_rule')\n",
        "        return rule\n",
        "\n",
        "    if not eval_index_on_ruleset:\n",
        "\n",
        "        # Currently-best pruned rule and its prune value\n",
        "        best_rule = copy.deepcopy(rule)\n",
        "        best_v = 0\n",
        "\n",
        "        # Iterative test rule\n",
        "        current_rule = copy.deepcopy(rule)\n",
        "\n",
        "        while current_rule.conds:\n",
        "            v = prune_metric(current_rule, pos_pruneset, neg_pruneset)\n",
        "            if verbosity >= 5:\n",
        "                print(f\"prune value of {current_rule}: {rnd(v)}\")\n",
        "            if v is None:\n",
        "                return None\n",
        "            if v >= best_v:\n",
        "                best_v = v\n",
        "                best_rule = copy.deepcopy(current_rule)\n",
        "            current_rule.conds.pop(-1)\n",
        "\n",
        "        if verbosity >= 2:\n",
        "            if len(best_rule.conds) != len(rule.conds):\n",
        "                print(f\"pruned rule: {best_rule}\")\n",
        "            else:\n",
        "                print(f\"pruned rule unchanged\")\n",
        "        return best_rule\n",
        "\n",
        "    else:\n",
        "        # Check if index matches rule to prune\n",
        "        rule_index, ruleset = eval_index_on_ruleset\n",
        "        if ruleset.rules[rule_index] != rule:\n",
        "            raise ValueError(\n",
        "                f\"rule mismatch: {rule} - {ruleset.rules[rule_index]} in {ruleset}\"\n",
        "            )\n",
        "\n",
        "        current_ruleset = copy.deepcopy(ruleset)\n",
        "        current_rule = current_ruleset.rules[rule_index]\n",
        "        best_ruleset = copy.deepcopy(current_ruleset)\n",
        "        best_v = 0\n",
        "\n",
        "        # Iteratively prune and test rule over ruleset.\n",
        "        # This is unfortunately expensive.\n",
        "        while current_rule.conds:\n",
        "            v = prune_metric(current_ruleset, pos_pruneset, neg_pruneset)\n",
        "            if verbosity >= 5:\n",
        "                print(f\"prune value of {current_rule}: {rnd(v)}\")\n",
        "            if v is None:\n",
        "                return None\n",
        "            if v >= best_v:\n",
        "                best_v = v\n",
        "                best_rule = copy.deepcopy(current_rule)\n",
        "                best_ruleset = copy.deepcopy(current_ruleset)\n",
        "            current_rule.conds.pop(-1)\n",
        "            current_ruleset.rules[rule_index] = current_rule\n",
        "        return best_rule\n",
        "\n",
        "\n",
        "def prune_rule_cn(\n",
        "    cn, rule, prune_metric_cn, pos_idx, neg_idx, eval_index_on_ruleset=None, verbosity=0\n",
        "):\n",
        "    \n",
        "    if rule.isempty():\n",
        "        # warning_str = f\"can't prune empty rule: {rule}\"\n",
        "        # _warn(warning_str, RuntimeWarning, filename='base_functions', funcname='prune_rule_cn')\n",
        "        return rule\n",
        "\n",
        "    if not eval_index_on_ruleset:\n",
        "\n",
        "        # Currently-best pruned rule and its prune value\n",
        "        best_rule = copy.deepcopy(rule)\n",
        "        best_v = 0\n",
        "\n",
        "        # Iterative test rule\n",
        "        current_rule = copy.deepcopy(rule)\n",
        "\n",
        "        while current_rule.conds:\n",
        "            v = prune_metric_cn(cn, current_rule, pos_idx, neg_idx)\n",
        "            if verbosity >= 5:\n",
        "                print(f\"prune value of {current_rule}: {rnd(v)}\")\n",
        "            if v is None:\n",
        "                return None\n",
        "            if v >= best_v:\n",
        "                best_v = v\n",
        "                best_rule = copy.deepcopy(current_rule)\n",
        "            current_rule.conds.pop(-1)\n",
        "\n",
        "        if verbosity >= 2:\n",
        "            if len(best_rule.conds) != len(rule.conds):\n",
        "                print(f\"pruned rule: {best_rule}\")\n",
        "            else:\n",
        "                print(f\"pruned rule unchanged\")\n",
        "        return best_rule\n",
        "\n",
        "    # cn is Untouched below here\n",
        "    else:\n",
        "        # Check if index matches rule to prune\n",
        "        rule_index, ruleset = eval_index_on_ruleset\n",
        "        if ruleset.rules[rule_index] != rule:\n",
        "            raise ValueError(\n",
        "                f\"rule mismatch: {rule} - {ruleset.rules[rule_index]} in {ruleset}\"\n",
        "            )\n",
        "\n",
        "        current_ruleset = copy.deepcopy(ruleset)\n",
        "        current_rule = current_ruleset.rules[rule_index]\n",
        "        best_ruleset = copy.deepcopy(current_ruleset)\n",
        "        best_v = 0\n",
        "\n",
        "        # Iteratively prune and test rule over ruleset.\n",
        "        while current_rule.conds:\n",
        "            v = prune_metric_cn(cn, current_rule, pos_idx, neg_idx)\n",
        "            if verbosity >= 5:\n",
        "                print(f\"prune value of {current_rule}: {rnd(v)}\")\n",
        "            if v is None:\n",
        "                return None\n",
        "            if v >= best_v:\n",
        "                best_v = v\n",
        "                best_rule = copy.deepcopy(current_rule)\n",
        "                best_ruleset = copy.deepcopy(current_ruleset)\n",
        "            current_rule.conds.pop(-1)\n",
        "            current_ruleset.rules[rule_index] = current_rule\n",
        "        return best_rule\n",
        "\n",
        "\n",
        "def recalibrate_proba(\n",
        "    ruleset, Xy_df, class_feat, pos_class, min_samples=10, require_min_samples=True\n",
        "):\n",
        "    \n",
        "\n",
        "    # At least this many samples per rule (or neg) must be of correct class\n",
        "    required_correct_samples = 1\n",
        "\n",
        "    # If not using min_samples, set it to 1\n",
        "    if not min_samples or min_samples < 1:\n",
        "        min_samples = 1\n",
        "\n",
        "    # Collect each Rule's pos and neg frequencies in list \"rule_class_freqs\"\n",
        "    # Store rules that lack enough samples in list \"insufficient_rules\"\n",
        "    df = Xy_df\n",
        "\n",
        "    rule_class_freqs = [None] * len(ruleset.rules)\n",
        "    insufficient_rules = []\n",
        "    for i, rule in enumerate(ruleset.rules):\n",
        "        npos_pred = num_pos(rule.covers(df), class_feat=class_feat, pos_class=pos_class)\n",
        "        nneg_pred = num_neg(rule.covers(df), class_feat=class_feat, pos_class=pos_class)\n",
        "        neg_pos_pred = (nneg_pred, npos_pred)\n",
        "        rule_class_freqs[i] = neg_pos_pred\n",
        "        # Rule has insufficient samples if fewer than minsamples or lacks at least one correct sample\n",
        "        if (\n",
        "            sum(neg_pos_pred) < min_samples\n",
        "            or sum(neg_pos_pred) < 1\n",
        "            or neg_pos_pred[0] < required_correct_samples\n",
        "        ):\n",
        "            insufficient_rules.append(rule)\n",
        "\n",
        "    # Collect class frequencies for negative predictions\n",
        "    uncovered = df.drop(ruleset.covers(df).index)\n",
        "    neg_freq = num_neg(uncovered, class_feat=class_feat, pos_class=pos_class)\n",
        "    tn_fn = (neg_freq, len(uncovered) - neg_freq)\n",
        "\n",
        "    # Issue warnings if trouble with sample size\n",
        "    if require_min_samples:\n",
        "        if insufficient_rules:  # WARN if/which rules lack enough samples\n",
        "            pretty_insufficient_rules = \"\\n\".join([str(r) for r in insufficient_rules])\n",
        "            warning_str = f\"param min_samples={min_samples}; insufficient number of samples or fewer than {required_correct_samples} correct samples for rules {pretty_insufficient_rules}\"\n",
        "            _warn(\n",
        "                warning_str,\n",
        "                RuntimeWarning,\n",
        "                filename=\"base_functions\",\n",
        "                funcname=\"recalibrate_proba\",\n",
        "            )\n",
        "        if neg_freq < min_samples or tn_fn[1] < 1:  # WARN if neg lacks enough samples\n",
        "            warning_str = f\"param min_samples={min_samples}; insufficient number of negatively labled samples\"\n",
        "            _warn(\n",
        "                warning_str,\n",
        "                RuntimeWarning,\n",
        "                filename=\"base_functions\",\n",
        "                funcname=\"recalibrate_proba\",\n",
        "            )\n",
        "        if insufficient_rules or sum(tn_fn) < min_samples:\n",
        "            if (\n",
        "                require_min_samples\n",
        "            ):  # WARN if require_min_samples -> halting recalibration\n",
        "                warning_str = f\"Recalibrating halted. to recalibrate, try using more samples, lowering min_samples, or set require_min_samples to False\"\n",
        "                _warn(\n",
        "                    warning_str,\n",
        "                    RuntimeWarning,\n",
        "                    filename=\"base_functions\",\n",
        "                    funcname=\"recalibrate_proba\",\n",
        "                )\n",
        "                return\n",
        "            else:  # GO AHEAD EVEN THOUGH NOT ENOUGH SAMPLES\n",
        "                pass\n",
        "                # warning_str = f'Because require_min_samples=False, recalibrating probabilities for any rules with enough samples min_samples>={min_samples} that have at least {required_correct_samples} correct samples even though not all rules have enough samples. Probabilities for any rules that lack enough samples will be retained.'\n",
        "                # _warn(warning_str, RuntimeWarning, filename='base_functions', funcname='recalibrate_proba')\n",
        "\n",
        "    # Assign collected frequencies to Rules\n",
        "    for rule, freqs in zip(ruleset.rules, rule_class_freqs):\n",
        "        if sum(freqs) >= min_samples and freqs[0] >= required_correct_samples:\n",
        "            rule.class_freqs = freqs\n",
        "        else:\n",
        "            rule.class_freqs = None\n",
        "\n",
        "    # Assign Ruleset's uncovered frequencies\n",
        "    if not hasattr(ruleset, \"uncovered_class_freqs\") or (\n",
        "        neg_freq >= min_samples and tn_fn[1] >= required_correct_samples\n",
        "    ):\n",
        "        ruleset.uncovered_class_freqs = tn_fn\n",
        "\n",
        "    # Warn if no neg samples\n",
        "    if (\n",
        "        sum([freqs[0] for freqs in rule_class_freqs]) + ruleset.uncovered_class_freqs[0]\n",
        "        == 0\n",
        "    ):\n",
        "        _warn_only_single_class(\n",
        "            only_value=1,\n",
        "            pos_class=1,\n",
        "            filename=\"base_functions\",\n",
        "            funcname=\"recalibrate_proba\",\n",
        "        )\n",
        "    # Warn if no pos samples\n",
        "    elif (\n",
        "        sum([freqs[1] for freqs in rule_class_freqs]) + ruleset.uncovered_class_freqs[1]\n",
        "        == 0\n",
        "    ):\n",
        "        _warn_only_single_class(\n",
        "            only_value=0,\n",
        "            pos_class=1,\n",
        "            filename=\"base_functions\",\n",
        "            funcname=\"recalibrate_proba\",\n",
        "        )\n",
        "\n",
        "    ###################\n",
        "    ##### METRICS #####\n",
        "    ###################\n",
        "\n",
        "\n",
        "def gain(before, after, pos_df, neg_df):\n",
        "    \"\"\"Calculates the information gain from before to after.\"\"\"\n",
        "    p0count = before.num_covered(pos_df) # tp\n",
        "    p1count = after.num_covered(pos_df) # tp after action step\n",
        "    n0count = before.num_covered(neg_df) # fn\n",
        "    n1count = after.num_covered(neg_df) # fn after action step\n",
        "    return p1count * (\n",
        "        math.log2((p1count + 1) / (p1count + n1count + 1))\n",
        "        - math.log2((p0count + 1) / (p0count + n0count + 1))\n",
        "    )\n",
        "\n",
        "\n",
        "def gain_cn(cn, cond_step, rule_covers_pos_idx, rule_covers_neg_idx):\n",
        "    \"\"\"Calculates the information gain from adding a Cond.\"\"\"\n",
        "    p0count = len(rule_covers_pos_idx) # tp\n",
        "    p1count = len(cn.cond_covers(cond_step, subset=rule_covers_pos_idx)) # tp after action step\n",
        "    n0count = len(rule_covers_neg_idx) # fn\n",
        "    n1count = len(cn.cond_covers(cond_step, subset=rule_covers_neg_idx)) # fn after action step\n",
        "    return p1count * (\n",
        "        math.log2((p1count + 1) / (p1count + n1count + 1))\n",
        "        - math.log2((p0count + 1) / (p0count + n0count + 1))\n",
        "    )\n",
        "\n",
        "\n",
        "def precision(object, pos_df, neg_df):\n",
        "    \"\"\"Calculate precision value of object's classification.\n",
        "    object : Cond, Rule, or Ruleset\n",
        "    \"\"\"\n",
        "\n",
        "    pos_covered = object.covers(pos_df)\n",
        "    neg_covered = object.covers(neg_df)\n",
        "    total_n_covered = len(pos_covered) + len(neg_covered)\n",
        "    if total_n_covered == 0:\n",
        "        return None\n",
        "    else:\n",
        "        return len(pos_covered) / total_n_covered\n",
        "\n",
        "\n",
        "def rule_precision_cn(cn, rule, pos_idx, neg_idx):\n",
        "    pos_covered = cn.rule_covers(rule, pos_idx)\n",
        "    neg_covered = cn.rule_covers(rule, neg_idx)\n",
        "    total_n_covered = len(pos_covered) + len(neg_covered)\n",
        "    if total_n_covered == 0:\n",
        "        return None\n",
        "    else:\n",
        "        return len(pos_covered) / total_n_covered\n",
        "\n",
        "\n",
        "def score_accuracy(predictions, actuals):\n",
        "  \n",
        "    t = [pr for pr, act in zip(predictions, actuals) if pr == act]\n",
        "    n = predictions\n",
        "    return len(t) / len(n)\n",
        "\n",
        "\n",
        "def _accuracy(object, pos_pruneset, neg_pruneset):\n",
        "   \n",
        "    P = len(pos_pruneset)\n",
        "    N = len(neg_pruneset)\n",
        "    if P + N == 0:\n",
        "        return None\n",
        "\n",
        "    tp = len(object.covers(pos_pruneset))\n",
        "    tn = N - len(object.covers(neg_pruneset))\n",
        "    return (tp + tn) / (P + N)\n",
        "\n",
        "\n",
        "def _rule_accuracy_cn(cn, rule, pos_pruneset_idx, neg_pruneset_idx):\n",
        "    \n",
        "    P = len(pos_pruneset_idx)\n",
        "    N = len(neg_pruneset_idx)\n",
        "    if P + N == 0:\n",
        "        return None\n",
        "\n",
        "    tp = len(cn.rule_covers(rule, pos_pruneset_idx))\n",
        "    tn = N - len(cn.rule_covers(rule, neg_pruneset_idx))\n",
        "    return (tp + tn) / (P + N)\n",
        "\n",
        "\n",
        "def best_successor(rule, possible_conds, pos_df, neg_df, verbosity=0):\n",
        "    \"\"\"Return for a Rule its best successor Rule according to FOIL information gain metric.\"\"\"\n",
        "\n",
        "    best_gain = 0\n",
        "    best_successor_rule = None\n",
        "\n",
        "    for successor in rule.successors(possible_conds, pos_df, neg_df):\n",
        "        g = gain(rule, successor, pos_df, neg_df)\n",
        "        if g > best_gain:\n",
        "            best_gain = g\n",
        "            best_successor_rule = successor\n",
        "    if verbosity >= 5:\n",
        "        print(f\"gain {rnd(best_gain)} {best_successor_rule}\")\n",
        "    return best_successor_rule\n",
        "\n",
        "\n",
        "def best_rule_successor_cn(cn, rule, pos_idx, neg_idx, verbosity=0):\n",
        "    \"\"\"Return for a Rule its best successor Rule according to FOIL information gain metric.\"\"\"\n",
        "\n",
        "    best_cond = None\n",
        "    best_gain = float(\"-inf\")\n",
        "\n",
        "    rule_covers_pos_idx = cn.rule_covers(rule, pos_idx)\n",
        "    rule_covers_neg_idx = cn.rule_covers(rule, neg_idx)\n",
        "\n",
        "    for cond_action_step in cn.conds:\n",
        "        g = gain_cn(cn, cond_action_step, rule_covers_pos_idx, rule_covers_neg_idx)\n",
        "        if g > best_gain:\n",
        "            best_gain = g\n",
        "            best_cond = cond_action_step\n",
        "    if verbosity >= 5:\n",
        "        print(f\"gain {rnd(best_gain)} {best_cond}\")\n",
        "    return Rule(rule.conds + [best_cond]) if best_gain > 0 else None\n",
        "\n",
        "\n",
        "###################\n",
        "##### HELPERS #####\n",
        "###################\n",
        "\n",
        "\n",
        "def pos_neg_split(df, class_feat, pos_class):\n",
        "    \"\"\"Split df into pos and neg classes.\"\"\"\n",
        "    pos_df = pos(df, class_feat, pos_class)\n",
        "    neg_df = neg(df, class_feat, pos_class)\n",
        "    return pos_df, neg_df\n",
        "\n",
        "\n",
        "def df_shuffled_split(df, split_size=0.66, random_state=None):\n",
        "   \n",
        "    idx1, idx2 = random_split(\n",
        "        df.index, split_size, res_type=set, random_state=random_state\n",
        "    )\n",
        "    return df.loc[idx1, :], df.loc[idx2, :]\n",
        "\n",
        "\n",
        "def set_shuffled_split(set_to_split, split_size, random_state=None):\n",
        "   \n",
        "    list_to_split = list(set_to_split)\n",
        "    seed(random_state)\n",
        "    shuffle(list_to_split)\n",
        "    split_at = int(len(list_to_split) * split_size)\n",
        "    return (set(list_to_split[:split_at]), set(list_to_split[split_at:]))\n",
        "\n",
        "\n",
        "def random_split(to_split, split_size, res_type=set, random_state=None):\n",
        "  \n",
        "    to_split = list(to_split)\n",
        "    seed(random_state)\n",
        "    shuffle(to_split)\n",
        "    split_at = int(len(to_split) * split_size)\n",
        "    return (res_type(to_split[:split_at]), res_type(to_split[split_at:]))\n",
        "\n",
        "\n",
        "def pos(df, class_feat, pos_class):\n",
        "    \"\"\"Return subset of instances that are labeled positive.\"\"\"\n",
        "    return df[df[class_feat] == pos_class]\n",
        "\n",
        "\n",
        "def neg(df, class_feat, pos_class):\n",
        "    \"\"\"Return subset of instances that are labeled negative.\"\"\"\n",
        "    return df[df[class_feat] != pos_class]\n",
        "\n",
        "\n",
        "def num_pos(df, class_feat, pos_class):\n",
        "    \"\"\"Return number of instances that are labeled positive.\"\"\"\n",
        "    return len(df[df[class_feat] == pos_class])\n",
        "\n",
        "\n",
        "def num_neg(df, class_feat, pos_class):\n",
        "    \"\"\" Return number of instances that are labeled negative.\"\"\"\n",
        "    return len(df[df[class_feat] != pos_class])\n",
        "\n",
        "\n",
        "def nCr(n, r):\n",
        "    \"\"\"Return number of combinations C(n, r).\"\"\"\n",
        "\n",
        "    def product(numbers):\n",
        "        return reduce(op.mul, numbers, 1)\n",
        "\n",
        "    num = product(range(n, n - r, -1))\n",
        "    den = product(range(1, r + 1))\n",
        "    return num // den\n",
        "\n",
        "\n",
        "def argmin(iterable):\n",
        "    \"\"\"Return index of minimum value.\"\"\"\n",
        "    lowest_val = iterable[0]\n",
        "    lowest_i = 0\n",
        "    for i, val in enumerate(iterable):\n",
        "        if val < lowest_val:\n",
        "            lowest_val = val\n",
        "            lowest_i = i\n",
        "    return lowest_i\n",
        "\n",
        "\n",
        "def i_replaced(list_, i, value):\n",
        "   \n",
        "    if value is not None:\n",
        "        return list_[:i] + [value] + list_[i + 1 :]\n",
        "    else:\n",
        "        return list_[:i] + list_[i + 1 :]\n",
        "\n",
        "\n",
        "def rm_covered(object, pos_df, neg_df):\n",
        "    \n",
        "    return (\n",
        "        pos_df.drop(object.covers(pos_df).index, axis=0, inplace=False),\n",
        "        neg_df.drop(object.covers(neg_df).index, axis=0, inplace=False),\n",
        "    )\n",
        "\n",
        "\n",
        "def rm_rule_covers_cn(cn, rule, pos_idx, neg_idx):\n",
        "    \"\"\"Return positive and negative indices not covered by object.\"\"\"\n",
        "    return (\n",
        "        pos_idx - cn.rule_covers(rule, pos_idx),\n",
        "        neg_idx - cn.rule_covers(rule, neg_idx),\n",
        "    )\n",
        "\n",
        "\n",
        "def truncstr(iterable, limit=5, direction=\"left\"):\n",
        "    \n",
        "    if len(iterable) > limit:\n",
        "        if direction == \"left\":\n",
        "            return iterable[:limit].__str__() + \"...\"\n",
        "        elif direction == \"right\":\n",
        "            return \"...\" + iterable[-limit:].__str__()\n",
        "        else:\n",
        "            raise ValueError('direction param must be \"left\" or \"right\"')\n",
        "    else:\n",
        "        return str(iterable)\n",
        "\n",
        "\n",
        "def stop_early(ruleset, max_rules, max_total_conds):\n",
        "    \"\"\"Function to decide whether to halt training.\"\"\"\n",
        "    return (max_rules is not None and len(ruleset.rules) >= max_rules) or (\n",
        "        max_total_conds is not None and ruleset.count_conds() >= max_total_conds\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgcUSOASfiUF"
      },
      "source": [
        "#discretize.py\n",
        "\n",
        "class BinTransformer:\n",
        "    def __init__(self, n_discretize_bins=10, names_precision=2, verbosity=0):\n",
        "        self.n_discretize_bins = n_discretize_bins\n",
        "        self.names_precision = names_precision\n",
        "        self.verbosity = verbosity\n",
        "        self.bins_ = None\n",
        "\n",
        "    def __str__(self):\n",
        "        return str(self.bins_)\n",
        "\n",
        "    __repr__ = __str__\n",
        "\n",
        "    def __bool__(self):\n",
        "        return not not self.bins_\n",
        "\n",
        "    def isempty(self):\n",
        "        return not self.bins_ is None and not self.bins_\n",
        "\n",
        "    def fit_or_fittransform_(self, df, ignore_feats=[]):\n",
        "        \"\"\"Transform df using pre-fit bins, or, if unfit, fit self and transform df\"\"\"\n",
        "\n",
        "        # Binning has already been fit\n",
        "        if self.bins_:\n",
        "            return self.transform(df)\n",
        "\n",
        "        # Binning disabled\n",
        "        elif not self.n_discretize_bins:\n",
        "            return df\n",
        "\n",
        "        # Binning enabled, and binner needs to be fit\n",
        "        else:\n",
        "            self.fit(df, ignore_feats=ignore_feats)\n",
        "            df, bins = self.transform(df, ignore_feats=ignore_feats)\n",
        "            self.bins = bins\n",
        "            return df\n",
        "\n",
        "    def fit_transform(self, df, ignore_feats=[]):\n",
        "        self.fit(df, ignore_feats=ignore_feats)\n",
        "        return self.transform(df)\n",
        "\n",
        "    def transform(self, df, ignore_feats=[]):\n",
        "        \"\"\"Return df with seemingly continuous features binned, and the bin_transformer or None depending on whether binning occurs.\"\"\"\n",
        "\n",
        "        if n_discretize_bins is None:\n",
        "            return df\n",
        "\n",
        "        if self.bins_ == {}:\n",
        "            return df\n",
        "\n",
        "        isbinned = False\n",
        "        continuous_feats = find_continuous_feats(df, ignore_feats=ignore_feats)\n",
        "        if self.n_discretize_bins:\n",
        "            if continuous_feats:\n",
        "                if self.verbosity == 1:\n",
        "                    print(f\"binning data...\\n\")\n",
        "                elif self.verbosity >= 2:\n",
        "                    print(f\"binning features {continuous_feats}...\")\n",
        "                binned_df = df.copy()\n",
        "                bin_transformer = fit_bins(\n",
        "                    binned_df, output=False, ignore_feats=ignore_feats,\n",
        "                )\n",
        "                binned_df = bin_transform(binned_df, bin_transformer)\n",
        "                isbinned = True\n",
        "        else:\n",
        "            n_unique_values = sum(\n",
        "                [len(u) for u in [df[f].unique() for f in continuous_feats]]\n",
        "            )\n",
        "            warning_str = f\"There are {len(continuous_feats)} features to be treated as continuous: {continuous_feats}. \\n Treating {n_unique_values} numeric values as nominal or discrete. To auto-discretize features, assign a value to parameter 'n_discretize_bins.'\"\n",
        "            _warn(warning_str, RuntimeWarning, filename=\"base\", funcname=\"transform\")\n",
        "        if isbinned:\n",
        "            self.bins_ = bin_transformer\n",
        "            return binned_df, bin_transformer\n",
        "        else:\n",
        "            return df\n",
        "\n",
        "    def find_continuous_feats(self, df, ignore_feats=[]):\n",
        "        \"\"\"Return names of df features that seem to be continuous.\"\"\"\n",
        "\n",
        "        if not self.n_discretize_bins:\n",
        "            return []\n",
        "\n",
        "        # Find numeric features\n",
        "        cont_feats = df.select_dtypes(np.number).columns\n",
        "\n",
        "        # Remove discrete features\n",
        "        cont_feats = [\n",
        "            f for f in cont_feats if len(df[f].unique()) > self.n_discretize_bins\n",
        "        ]\n",
        "\n",
        "        # Remove ignore features\n",
        "        cont_feats = [f for f in cont_feats if f not in ignore_feats]\n",
        "\n",
        "        return cont_feats\n",
        "\n",
        "    def fit(self, df, output=False, ignore_feats=[]):\n",
        "        \n",
        "\n",
        "        def _fit_feat(df, feat):\n",
        "            \"\"\"Return list of tuples defining bin ranges for a numerical feature using simple linear search\"\"\"\n",
        "\n",
        "            if len(df) == 0:\n",
        "                return []\n",
        "\n",
        "            n_discretize_bins = min(\n",
        "                self.n_discretize_bins, len(df[feat].unique())\n",
        "            )  # In case there are fewer unique values than n_discretize_bins\n",
        "            bin_size = len(df) // n_discretize_bins\n",
        "            sorted_df = df.sort_values(by=[feat])\n",
        "            sorted_values = sorted_df[feat].tolist()\n",
        "\n",
        "            sizes = []  # for verbosity output\n",
        "            if self.verbosity >= 4:\n",
        "                print(\n",
        "                    f\"{feat}: fitting {len(df[feat].unique())} unique vals into {n_discretize_bins} bins\"\n",
        "                )\n",
        "\n",
        "            bin_ranges = []  # result\n",
        "            bin_num = 0  # current bin number\n",
        "\n",
        "            ceil_i = -1  # current bin ceiling index\n",
        "            ceil_val = None  # current bin upper bound\n",
        "\n",
        "            floor_i = 0  # current bin start index\n",
        "            floor_val = sorted_df.iloc[0][feat]  # current bin floor value\n",
        "\n",
        "            prev_finish_val = None  # prev bin upper bound\n",
        "            while bin_num < n_discretize_bins and floor_i < len(sorted_values):\n",
        "                # jump to tentative ceiling index\n",
        "                ceil_i = min(floor_i + bin_size, len(sorted_df) - 1)\n",
        "                ceil_val = sorted_df.iloc[ceil_i][feat]\n",
        "\n",
        "                # increment ceiling index until encounter a new value to ensure next bin size is correct\n",
        "                while (\n",
        "                    ceil_i < len(sorted_df) - 1  # not last bin\n",
        "                    and sorted_df.iloc[ceil_i][feat]\n",
        "                    == ceil_val  # keep looking for a new value\n",
        "                ):\n",
        "                    ceil_i += 1\n",
        "\n",
        "                # found ceiling index. update values\n",
        "                if self.verbosity >= 4:\n",
        "                    sizes.append(ceil_i - floor_i)\n",
        "                    print(\n",
        "                        f\"bin #{bin_num}, floor idx {floor_i} value: {sorted_df.iloc[floor_i][feat]}, ceiling idx {ceil_i} value: {sorted_df.iloc[ceil_i][feat]}\"\n",
        "                    )\n",
        "                bin_range = (floor_val, ceil_val)\n",
        "                bin_ranges.append(bin_range)\n",
        "\n",
        "                # update for next bin\n",
        "                floor_i = ceil_i + 1\n",
        "                floor_val = ceil_val\n",
        "                bin_num += 1\n",
        "\n",
        "            # Guarantee min and max values\n",
        "            bin_ranges[0] = (sorted_df.iloc[0][feat], bin_ranges[0][1])\n",
        "            bin_ranges[-1] = (bin_ranges[-1][0], sorted_df.iloc[-1][feat])\n",
        "\n",
        "            if self.verbosity >= 4:\n",
        "                print(f\"-bin sizes {sizes}; dataVMR={rnd(np.var(df[feat])/np.mean(df[feat]))}, binVMR={rnd(np.var(sizes)/np.mean(sizes))}\") \n",
        "            return bin_ranges\n",
        "\n",
        "        # Create dict to store fit definitions for each feature\n",
        "        fit_dict = {}\n",
        "        feats_to_fit = self.find_continuous_feats(df, ignore_feats=ignore_feats)\n",
        "        if self.verbosity == 2:\n",
        "            print(f\"fitting bins for features {feats_to_fit}\")\n",
        "        if self.verbosity >= 2:\n",
        "            print()\n",
        "\n",
        "        # Collect fits in dict\n",
        "        count = 1\n",
        "        for feat in feats_to_fit:\n",
        "            fit = _fit_feat(df, feat)\n",
        "            fit_dict[feat] = fit\n",
        "        self.bins_ = fit_dict\n",
        "\n",
        "    def transform(self, df):\n",
        "      \n",
        "\n",
        "        if self.bins_ is None:\n",
        "            return df\n",
        "\n",
        "        # Replace each feature with bin transformations\n",
        "        for feat, bin_fit_list in self.bins_.items():\n",
        "            if feat in df.columns:\n",
        "                df[feat] = df[feat].map(\n",
        "                    lambda x: self._transform_value(x, bin_fit_list)\n",
        "                )\n",
        "        return df\n",
        "\n",
        "    def _transform_value(self, value, bin_fit_list):\n",
        "        \"\"\"Return bin string name for a given numerical value. Assumes bin_fit_list is ordered.\"\"\"\n",
        "        min_val, min_bin = bin_fit_list[0][0], bin_fit_list[0]\n",
        "        max_val, max_bin = bin_fit_list[-1][1], bin_fit_list[-1]\n",
        "        for bin_fit in bin_fit_list:\n",
        "            if value <= bin_fit[1]:\n",
        "                start_name = (\n",
        "                    str(round(bin_fit[0], self.names_precision))\n",
        "                    if self.names_precision\n",
        "                    else str(int(bin_fit[0]))\n",
        "                )\n",
        "                finish_name = (\n",
        "                    str(round(bin_fit[1], self.names_precision))\n",
        "                    if self.names_precision\n",
        "                    else str(int(bin_fit[1]))\n",
        "                )\n",
        "                bin_name = \"-\".join([start_name, finish_name])\n",
        "                return bin_name\n",
        "        if value <= min_val:\n",
        "            return min_bin\n",
        "        elif value >= max_val:\n",
        "            return max_bin\n",
        "        else:\n",
        "            raise ValueError(\"No bin found for value\", value)\n",
        "\n",
        "    def _try_rename_features(self, df, class_feat, feature_names):\n",
        "        \"\"\"Rename df columns according to user request.\"\"\"\n",
        "        # Rename if same number of features\n",
        "        df_columns = [col for col in df.columns.tolist() if col != class_feat]\n",
        "        if len(df_columns) == len(feature_names):\n",
        "            col_replacements_dict = {\n",
        "                old: new for old, new in zip(df_columns, feature_names)\n",
        "            }\n",
        "            df = df.rename(columns=col_replacements_dict)\n",
        "            return df\n",
        "        # Wrong number of feature names\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "    def _construct_from_ruleset(self, ruleset):\n",
        "        MIN_N_DISCRETIZED_BINS = 10\n",
        "\n",
        "        bt = BinTransformer()\n",
        "        bt.bins_ = self._bin_prediscretized_features(ruleset)\n",
        "        bt.n_discretize_bins = max(\n",
        "            (MIN_N_DISCRETIZED_BINS, max(len(bins) for bins in bt.bins_.values()))\n",
        "        )\n",
        "        bt.names_precision = self._max_dec_precision(bt.bins_)\n",
        "        return bt\n",
        "\n",
        "    def _bin_prediscretized_features(self, ruleset):\n",
        "        def is_valid_decimal(s):\n",
        "            try:\n",
        "                float(s)\n",
        "            except:\n",
        "                return False\n",
        "            return True\n",
        "\n",
        "        def find_floor_ceil(value):\n",
        "            \"\"\"id min, max separated by a dash. Return None if invalid pattern.\"\"\"\n",
        "            split_idx = 0\n",
        "            for i, char in enumerate(value):\n",
        "                # Found a possible split and it's not the first number's minus sign\n",
        "                if char == \"-\" and i != 0:\n",
        "                    if split_idx is not None and not split_idx:\n",
        "                        split_idx = i\n",
        "                    # Found a - after the split, and it's not the minus of a negative number\n",
        "                    elif i > split_idx + 1:\n",
        "                        return None\n",
        "\n",
        "            floor = value[:split_idx]\n",
        "            ceil = value[split_idx + 1 :]\n",
        "            if is_valid_decimal(floor) and is_valid_decimal(ceil):\n",
        "                return (floor, ceil)\n",
        "            else:\n",
        "                return None\n",
        "\n",
        "        # Main function: _bin_prediscretized_features\n",
        "        discrete = defaultdict(list)\n",
        "        for cond in ruleset.get_conds():\n",
        "            floor_ceil = find_floor_ceil(cond.val)\n",
        "            if floor_ceil:\n",
        "                discrete[cond.feature].append(floor_ceil)\n",
        "        for feat, ranges in discrete.items():\n",
        "            ranges.sort(key=lambda x: float(x[0]))\n",
        "        return dict(discrete)\n",
        "\n",
        "    def _max_dec_precision(self, bins_dict):\n",
        "        def dec_precision(value):\n",
        "            try:\n",
        "                return len(value) - value.index(\".\") - 1\n",
        "            except:\n",
        "                return 0\n",
        "\n",
        "        max_prec = 0\n",
        "        for bins in bins_dict.values():\n",
        "            for bin_ in bins:\n",
        "                for value in bin_:\n",
        "                    cur_prec = dec_precision(value)\n",
        "                    if cur_prec > max_prec:\n",
        "                        max_prec = cur_prec\n",
        "        return max_prec"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oGZUlWxQWumd"
      },
      "source": [
        "#preprocess.py\n",
        "def preprocess_training_data(preprocess_params):\n",
        "\n",
        "    # Get params\n",
        "    trainset = preprocess_params[\"trainset\"]\n",
        "    y = preprocess_params[\"y\"]\n",
        "    class_feat = preprocess_params[\"class_feat\"]\n",
        "    pos_class = preprocess_params[\"pos_class\"]\n",
        "    user_requested_feature_names = preprocess_params[\"feature_names\"]\n",
        "    n_discretize_bins = preprocess_params[\"n_discretize_bins\"]\n",
        "    verbosity = preprocess_params[\"verbosity\"]\n",
        "\n",
        "    # Error check\n",
        "    _check_valid_input_data(\n",
        "        trainset,\n",
        "        y,\n",
        "        class_feat,\n",
        "        user_requested_feature_names=user_requested_feature_names,\n",
        "    )\n",
        "\n",
        "    # Determine class_feat\n",
        "    class_feat = _get_class_feat_name(class_feat, y)\n",
        "\n",
        "    # Build new DataFrame containing both X and y.\n",
        "    df = _convert_to_training_df(\n",
        "        trainset,\n",
        "        y,\n",
        "        class_feat,\n",
        "        user_requested_feature_names=user_requested_feature_names,\n",
        "    )\n",
        "\n",
        "    # Define pos_class\n",
        "    pos_class = _get_pos_class(df, class_feat, pos_class)\n",
        "\n",
        "    # Infer correct datatypes\n",
        "    df = df.infer_objects()\n",
        "\n",
        "    # Bin, if necessary\n",
        "    bin_transformer_ = BinTransformer(\n",
        "        n_discretize_bins=n_discretize_bins, verbosity=verbosity\n",
        "    )\n",
        "    df = bin_transformer_.fit_transform(df, ignore_feats=[class_feat])\n",
        "\n",
        "    # Done\n",
        "    return df, class_feat, pos_class, bin_transformer_\n",
        "\n",
        "\n",
        "def preprocess_prediction_data(preprocess_params):\n",
        "\n",
        "    X = preprocess_params[\"X\"]\n",
        "    class_feat = preprocess_params[\"class_feat\"]\n",
        "    pos_class = preprocess_params[\"pos_class\"]\n",
        "    user_requested_feature_names = preprocess_params[\"user_requested_feature_names\"]\n",
        "    selected_features_ = preprocess_params[\"selected_features_\"]\n",
        "    trainset_features_ = preprocess_params[\"trainset_features_\"]\n",
        "    bin_transformer_ = preprocess_params[\"bin_transformer_\"]\n",
        "    verbosity = preprocess_params[\"verbosity\"]\n",
        "\n",
        "    # Error check\n",
        "    _check_valid_input_data(\n",
        "        X,\n",
        "        y=None,\n",
        "        class_feat=class_feat,\n",
        "        requires_label=False,\n",
        "        user_requested_feature_names=user_requested_feature_names,\n",
        "    )\n",
        "\n",
        "    # Build new DataFrame containing both X and y.\n",
        "    df = _convert_to_prediction_df(\n",
        "        X,\n",
        "        class_feat=class_feat,\n",
        "        user_requested_feature_names=user_requested_feature_names,\n",
        "    )\n",
        "\n",
        "    # Make sure selected features are present\n",
        "    _check_model_features_present(df, selected_features_)\n",
        "\n",
        "    # Infer correct datatypes\n",
        "    df = df.infer_objects()\n",
        "\n",
        "    # Bin, if necessary\n",
        "    if bin_transformer_:\n",
        "        df = bin_transformer_.transform(df)\n",
        "\n",
        "    # Done\n",
        "    return df\n",
        "\n",
        "\n",
        "def _preprocess_recalibrate_proba_data(preprocess_params):\n",
        "\n",
        "    # Get params\n",
        "    X_or_Xy = preprocess_params[\"X_or_Xy\"]\n",
        "    y = preprocess_params[\"y\"]\n",
        "    class_feat = preprocess_params[\"class_feat\"]\n",
        "    pos_class = preprocess_params[\"pos_class\"]\n",
        "    user_requested_feature_names = preprocess_params[\"user_requested_feature_names\"]\n",
        "    bin_transformer_ = preprocess_params[\"bin_transformer_\"]\n",
        "    verbosity = preprocess_params[\"verbosity\"]\n",
        "\n",
        "    # Error check\n",
        "    _check_valid_input_data(\n",
        "        X_or_Xy,\n",
        "        y,\n",
        "        class_feat,\n",
        "        user_requested_feature_names=user_requested_feature_names,\n",
        "    )\n",
        "\n",
        "    # Build new DataFrame containing both X and y.\n",
        "    df = _convert_to_training_df(\n",
        "        X_or_Xy,\n",
        "        y,\n",
        "        class_feat,\n",
        "        user_requested_feature_names=user_requested_feature_names,\n",
        "    )\n",
        "\n",
        "    # Infer correct datatypes\n",
        "    df = df.infer_objects()\n",
        "\n",
        "    # Bin, if necessary\n",
        "    if bin_transformer_:\n",
        "        df = bin_transformer_.transform(df)\n",
        "\n",
        "    # Done\n",
        "    return df\n",
        "\n",
        "\n",
        "def _preprocess_y_score_data(y):\n",
        "    \"\"\"Return python iterable of y values.\"\"\"\n",
        "\n",
        "    def raise_wrong_ndim():\n",
        "        raise IndexError(f\"y input data has wrong number dimensions. It should have 1.\")\n",
        "\n",
        "    # If it's pandas or np...\n",
        "    if hasattr(y, \"ndim\"):\n",
        "        if y.ndim == 1:\n",
        "            return y.tolist()\n",
        "        else:\n",
        "            raise_wrong_ndim()\n",
        "\n",
        "    # Otherwise try for python iterable\n",
        "    if hasattr(y, \"__iter__\"):  # it's an iterable\n",
        "        # No super clean way to check for 1D, but this should be pretty decent\n",
        "        if any([hasattr(item, \"__iter__\") and type(item) is not str for item in y]):\n",
        "            raise_wrong_ndim()\n",
        "        else:\n",
        "            return y\n",
        "\n",
        "    # Otherwise, no idea what's going on\n",
        "    else:\n",
        "        raise TypeError(\n",
        "            f\"Could not identify valid type for y input data: {type(y)}. Recommended types are 1D python iterable, pandas Series, or 1D numpy array.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def _check_valid_input_data(\n",
        "    X_or_Xy,\n",
        "    y=None,\n",
        "    class_feat=None,\n",
        "    user_requested_feature_names=None,\n",
        "    requires_label=True,\n",
        "):\n",
        "\n",
        "    # Make sure there is data\n",
        "    if not _check_any_datasets_not_empty([X_or_Xy]):\n",
        "        raise ValueError(\"No data provided!\")\n",
        "\n",
        "    # If labels aren't needed, we're done\n",
        "    if not requires_label:\n",
        "        return\n",
        "\n",
        "    # Ensure class feature is provided\n",
        "    if (y is None) and (class_feat is None):\n",
        "        raise ValueError(\"y or class_feat param is required\")\n",
        "\n",
        "    # Ensure target data exists if class feat is provided\n",
        "    if y is None:\n",
        "        if user_requested_feature_names is not None:\n",
        "            feature_names = user_requested_feature_names\n",
        "        elif hasattr(X_or_Xy, \"columns\"):\n",
        "            feature_names = X_or_Xy.columns\n",
        "        else:\n",
        "            feature_names = list(range(len(X_or_Xy[0])))\n",
        "\n",
        "        if class_feat not in feature_names:\n",
        "            raise IndexError(\n",
        "                f\"Dataset does not include class feature name {class_feat}. Training set features: {feature_names}\"\n",
        "            )\n",
        "\n",
        "    # If both y and class_feat provided, ensure no name mismatch between them.\n",
        "    if (\n",
        "        y is not None\n",
        "        and class_feat is not None\n",
        "        and hasattr(y, \"name\")\n",
        "        and y.name != class_feat\n",
        "    ):\n",
        "        raise NameError(\n",
        "            f\"Feature name mismatch between params y {y.name} and class_feat {class_feat}. Besides, you only need to provide one of them.\"\n",
        "        )\n",
        "\n",
        "\n",
        "def _convert_to_training_df(X_or_Xy, y, class_feat, user_requested_feature_names=None):\n",
        "    \"\"\"Make a labeled Xy DataFrame from data.\"\"\"\n",
        "\n",
        "    # Create df from X_or_Xy\n",
        "    if isinstance(X_or_Xy, pd.DataFrame):\n",
        "        df = X_or_Xy.copy()\n",
        "    else:\n",
        "        df = pd.DataFrame(X_or_Xy)\n",
        "\n",
        "    # Set feature names\n",
        "    if user_requested_feature_names is not None:\n",
        "        df.columns = list(\n",
        "            user_requested_feature_names\n",
        "        )  # list in case the type is df.columns or something\n",
        "\n",
        "    # If necessary, merge y into df\n",
        "    if y is not None:\n",
        "        # If y is pd or np type, add it\n",
        "        try:\n",
        "            df = df.set_index(y.index)\n",
        "            df[class_feat] = y.copy()\n",
        "        # If that doesn't work, it's likely a python iterable\n",
        "        except:\n",
        "            df[class_feat] = y\n",
        "    return df\n",
        "\n",
        "\n",
        "def _convert_to_prediction_df(X_or_Xy, class_feat, user_requested_feature_names=None):\n",
        "    \"\"\"Make a labeled X DataFrame from data.\"\"\"\n",
        "\n",
        "    # Create df from X_or_Xy\n",
        "    if isinstance(X_or_Xy, pd.DataFrame):\n",
        "        df = X_or_Xy.copy()\n",
        "    else:\n",
        "        df = pd.DataFrame(X_or_Xy)\n",
        "\n",
        "    # Drop class feature if present\n",
        "    if class_feat in df.columns:\n",
        "        df.drop(class_feat, axis=1, inplace=True)\n",
        "\n",
        "    # Set feature names\n",
        "    if user_requested_feature_names:\n",
        "        df.columns = [f for f in user_requested_feature_names if not f == class_feat]\n",
        "    return df\n",
        "\n",
        "\n",
        "def _get_pos_class(df, class_feat, pos_class):\n",
        "    \"\"\"Get or infer the positive class name.\"\"\"\n",
        "    # Pos class already known\n",
        "\n",
        "    def raise_fail_infer_pos_class():\n",
        "        raise NameError(\n",
        "            f\"Couldn't infer name of positive target class from class feature: {class_feat}. Try using parameter pos_class to specify which class label should be treated as positive, or renaming your classes as booleans or 0, 1.\"\n",
        "        )\n",
        "\n",
        "    # pos class is already known\n",
        "    if pos_class is not None:\n",
        "        return pos_class\n",
        "\n",
        "    # Check if pos class can be inferred as True or 1\n",
        "    class_values = df[class_feat].unique()\n",
        "\n",
        "    # More than two classes\n",
        "    if len(class_values) > 2:\n",
        "        raise_fail_infer_pos_class()\n",
        "\n",
        "    # Only one class\n",
        "    elif len(class_values) == 1:\n",
        "        only_value = try_np_tonum(class_values[0])\n",
        "        if only_value is 0:\n",
        "            pos_class = 1\n",
        "        elif only_value is False:\n",
        "            pos_class = True\n",
        "        else:\n",
        "            pos_class = only_value\n",
        "        _warn_only_single_class(\n",
        "            only_value=only_value,\n",
        "            pos_class=pos_class,\n",
        "            filename=\"preprocess.py\",\n",
        "            funcname=\"_get_pos_class\",\n",
        "        )\n",
        "        return pos_class\n",
        "\n",
        "    # Exactly two class. Check if they are 01 or TrueFalse\n",
        "    else:\n",
        "        class_values.sort()\n",
        "        class_values = [try_np_tonum(val) for val in class_values]\n",
        "        if class_values[0] is 0 and class_values[1] is 1:\n",
        "            return 1\n",
        "        elif class_values[0] is False and class_values[1] is True:\n",
        "            return True\n",
        "\n",
        "    # Can't infer classes\n",
        "    raise_fail_infer_pos_class()\n",
        "\n",
        "\n",
        "def _get_class_feat_name(class_feat, y):\n",
        "\n",
        "    if class_feat is not None:\n",
        "        return class_feat\n",
        "\n",
        "    if y is not None and hasattr(y, \"name\"):\n",
        "        # If y is a pandas Series, try to get its name\n",
        "        class_feat = y.name\n",
        "    else:\n",
        "        # Create a name for it\n",
        "        class_feat = \"Class\"\n",
        "\n",
        "    return class_feat\n",
        "\n",
        "\n",
        "def _upgrade_bin_transformer_ifdepr(obj):\n",
        "    old_bin_transformer_ = getattr(obj, \"bin_transformer_\")\n",
        "    if type(old_bin_transformer_) == dict:\n",
        "        new_bin_transformer_ = BinTransformer()\n",
        "        new_bin_transformer_.bins_ = old_bin_transformer_\n",
        "        setattr(obj, \"bin_transformer_\", new_bin_transformer_)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9DZzqUcAX2vZ"
      },
      "source": [
        "# abstract ruleset classifier\n",
        "\n",
        "class AbstractRulesetClassifier(ABC):\n",
        "    def __init__(\n",
        "        self,\n",
        "        algorithm_name,\n",
        "        prune_size=0.33,\n",
        "        n_discretize_bins=10,\n",
        "        max_rules=None,\n",
        "        max_rule_conds=None,\n",
        "        max_total_conds=None,\n",
        "        random_state=None,\n",
        "        verbosity=0,\n",
        "    ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.VALID_HYPERPARAMETERS = {\n",
        "            \"prune_size\",\n",
        "            \"n_discretize_bins\",\n",
        "            \"max_rules\",\n",
        "            \"max_rule_conds\",\n",
        "            \"max_total_conds\",\n",
        "            \"random_state\",\n",
        "            \"verbosity\",\n",
        "        }\n",
        "        self.algorithm_name = algorithm_name\n",
        "        self.prune_size = prune_size\n",
        "        self.n_discretize_bins = n_discretize_bins\n",
        "        self.max_rules = max_rules\n",
        "        self.max_rule_conds = max_rule_conds\n",
        "        self.max_total_conds = max_total_conds\n",
        "        self.random_state = random_state\n",
        "        self.verbosity = verbosity\n",
        "\n",
        "        # This is to help keep sklearn ensemble happy should someone want use it\n",
        "        self._estimator_type = \"classifier\"\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Return string representation.\"\"\"\n",
        "        params = str(self.get_params()) + \">\"\n",
        "        params = (\n",
        "            params.replace(\": \", \"=\")\n",
        "            .replace(\"'\", \"\")\n",
        "            .replace(\"{\", \"(\")\n",
        "            .replace(\"}\", \")\")\n",
        "        )\n",
        "        return f\"<{self.algorithm_name}{params}\"\n",
        "\n",
        "    __repr__ = __str__\n",
        "\n",
        "    def out_model(self):\n",
        "        \"\"\"Print trained Ruleset model line-by-line: V represents 'or'; ^ represents 'and'.\"\"\"\n",
        "        if hasattr(self, \"ruleset_\"):\n",
        "            self.ruleset_.out_pretty()\n",
        "        else:\n",
        "            print(\"no model fitted\")\n",
        "\n",
        "    def predict(self, X, give_reasons=False, feature_names=None):\n",
        "        _check_is_model_fit(self)\n",
        "\n",
        "        _upgrade_bin_transformer_ifdepr(self)\n",
        "\n",
        "        # Preprocess prediction data\n",
        "        preprocess_params = {\n",
        "            \"X\": X,\n",
        "            \"class_feat\": self.class_feat,\n",
        "            \"pos_class\": self.pos_class,\n",
        "            \"bin_transformer_\": self.bin_transformer_,\n",
        "            \"user_requested_feature_names\": feature_names,\n",
        "            \"selected_features_\": self.selected_features_,\n",
        "            \"trainset_features_\": self.trainset_features_,\n",
        "            \"verbosity\": self.verbosity,\n",
        "        }\n",
        "\n",
        "        X_df = preprocess_prediction_data(preprocess_params)\n",
        "\n",
        "        return self.ruleset_.predict(X_df, give_reasons=give_reasons)\n",
        "\n",
        "    def score(self, X, y, score_function=score_accuracy):\n",
        "        _check_is_model_fit(self)\n",
        "        predictions = self.predict(X)\n",
        "        actuals = [\n",
        "            yi == self.pos_class for yi in _preprocess_y_score_data(y)\n",
        "        ]\n",
        "        return score_function(actuals, predictions)\n",
        "\n",
        "    def predict_proba(self, X, give_reasons=False, feature_names=None):\n",
        "        \n",
        "        _check_is_model_fit(self)\n",
        "\n",
        "        _upgrade_bin_transformer_ifdepr(self)\n",
        "\n",
        "        # Preprocess prediction data\n",
        "        preprocess_params = {\n",
        "            \"X\": X,\n",
        "            \"class_feat\": self.class_feat,\n",
        "            \"pos_class\": self.pos_class,\n",
        "            \"bin_transformer_\": self.bin_transformer_,\n",
        "            \"user_requested_feature_names\": feature_names,\n",
        "            \"selected_features_\": self.selected_features_,\n",
        "            \"trainset_features_\": self.trainset_features_,\n",
        "            \"verbosity\": self.verbosity,\n",
        "        }\n",
        "\n",
        "        X_df = preprocess_prediction_data(preprocess_params)\n",
        "\n",
        "        # This is to help keep sklearn ensemble happy should someone want use it\n",
        "        # self.classes_ = np.array([0, 1])\n",
        "        self.classes_ = np.array([f\"not {self.pos_class}\", self.pos_class])\n",
        "\n",
        "        return self.ruleset_.predict_proba(X_df, give_reasons=give_reasons)\n",
        "\n",
        "    def recalibrate_proba(\n",
        "        self,\n",
        "        X_or_Xy,\n",
        "        y=None,\n",
        "        feature_names=None,\n",
        "        min_samples=20,\n",
        "        require_min_samples=True,\n",
        "        discretize=True,\n",
        "    ):\n",
        "\n",
        "        # Preprocess training data\n",
        "        preprocess_params = {\n",
        "            \"X_or_Xy\": X_or_Xy,\n",
        "            \"y\": y,\n",
        "            \"class_feat\": self.class_feat,\n",
        "            \"pos_class\": self.pos_class,\n",
        "            \"bin_transformer_\": self.bin_transformer_ if discretize else None,\n",
        "            \"user_requested_feature_names\": feature_names,\n",
        "            \"min_samples\": min_samples,\n",
        "            \"require_min_samples\": require_min_samples,\n",
        "            \"verbosity\": self.verbosity,\n",
        "        }\n",
        "\n",
        "        df = _preprocess_recalibrate_proba_data(preprocess_params)\n",
        "\n",
        "        # Recalibrate\n",
        "        recalibrate_proba(\n",
        "            self.ruleset_,\n",
        "            Xy_df=df,\n",
        "            class_feat=self.class_feat,\n",
        "            pos_class=self.pos_class,\n",
        "            min_samples=min_samples,\n",
        "            require_min_samples=require_min_samples,\n",
        "        )\n",
        "\n",
        "    def copy(self):\n",
        "        \"\"\"Return deep copy of classifier.\"\"\"\n",
        "        return deepcopy(self)\n",
        "\n",
        "    def init_ruleset(self, ruleset=None):\n",
        "        if not ruleset:\n",
        "            self.ruleset_ = Ruleset()\n",
        "        else:\n",
        "            self.ruleset_ = asruleset(deepcopy(ruleset))\n",
        "\n",
        "    def set_ruleset(self, new_ruleset):\n",
        "        self.init_ruleset(new_ruleset)\n",
        "\n",
        "    def add_rule(self, new_rule):\n",
        "        self.ruleset_.add(new_rule)\n",
        "\n",
        "    def replace_rule_at(self, index, new_rule):\n",
        "        self.ruleset_.replace(index, new_rule)\n",
        "\n",
        "    def replace_rule(self, old_rule, new_rule):\n",
        "        self.ruleset_.replace_rule(old_rule, new_rule)\n",
        "\n",
        "    def remove_rule_at(self, index):\n",
        "        self.ruleset_.remove(index)\n",
        "\n",
        "    def remove_rule(self, old_rule):\n",
        "        self.ruleset_.remove_rule(old_rule)\n",
        "\n",
        "    def insert_rule_at(self, index, new_rule):\n",
        "        self.ruleset_.insert(index, new_rule)\n",
        "\n",
        "    def insert_rule(self, insert_before_rule, new_rule):\n",
        "        self.ruleset_.insert_rule(insert_before_rule, new_rule)\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        # parameter deep is a required artifact of sklearn compatability\n",
        "        return {param: self.__dict__.get(param) for param in self.VALID_HYPERPARAMETERS}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "    def _ruleset_frommodel(self, model):\n",
        "        \"\"\"Return the ruleset from model, which may be a Ruleset, wittgenstein classifier, or str.\"\"\"\n",
        "        if not model:\n",
        "            return Ruleset()\n",
        "        elif type(model) == Ruleset:\n",
        "            return deepcopy(model)\n",
        "        elif type(model) == str:\n",
        "            return deepcopy(asruleset(model))\n",
        "        elif isinstance(model, AbstractRulesetClassifier):\n",
        "            return deepcopy(model.ruleset_)\n",
        "        else:\n",
        "            raise AttributeError(f\"Couldnt recognize type: {type(model)} of model: {model}. Model should be of type Ruleset, str defining a ruleset, or wittgenstein classifier.\")\n",
        "\n",
        "    def _set_deprecated_fit_params(self, params):\n",
        "        \"\"\"Handle setting parameters passed to .fit that should have been passed to __init__\"\"\"\n",
        "        found_deprecated_params = []\n",
        "        for param, value in params.items():\n",
        "            if param in self.VALID_HYPERPARAMETERS:\n",
        "                found_deprecated_params.append(param)\n",
        "                setattr(self, param, value)\n",
        "        if found_deprecated_params:\n",
        "            _warn(\n",
        "                f\"In the future, you should assign these parameters when initializating classifier instead of during model fitting: {found_deprecated_params}\",\n",
        "                DeprecationWarning,\n",
        "                \"irep/ripper\",\n",
        "                \"fit\",\n",
        "            )\n",
        "\n",
        "    def to_csv(self, filename):\n",
        "        df = self._ruleset_to_df()\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "    def from_csv(self, filename, class_feat, pos_class):\n",
        "        model_df = pd.read_csv(filename)\n",
        "        self.ruleset_ = self._ruleset_from_df(model_df)\n",
        "        self._from_ruleset(self.ruleset_, class_feat, pos_class)\n",
        "\n",
        "    def to_txt(self, filename):\n",
        "        with open(filename, \"w+\") as f:\n",
        "            f.write(str(self.ruleset_))\n",
        "\n",
        "    def from_txt(self, filename, class_feat, pos_class):\n",
        "        with open(filename, \"r\") as f:\n",
        "            self.ruleset_ = ruleset_fromstr(f.read())\n",
        "        self._from_ruleset(self.ruleset_, class_feat, pos_class)\n",
        "\n",
        "    def _from_ruleset(self, ruleset, class_feat, pos_class):\n",
        "        self.ruleset_ = ruleset\n",
        "        self.trainset_features_ = self.ruleset_.get_selected_features()\n",
        "        self.selected_features_ = self.ruleset_.get_selected_features()\n",
        "        self.bin_transformer_ = BinTransformer()._construct_from_ruleset(self.ruleset_)\n",
        "        self.class_feat = class_feat\n",
        "        self.pos_class = pos_class\n",
        "\n",
        "    def _ruleset_from_df(self, model_df):\n",
        "        rules = []\n",
        "        for _, row in model_df.iterrows():\n",
        "            rules.append(Rule([cond_fromstr(c) for c in row if isinstance(c, str)]))\n",
        "        return Ruleset(rules)\n",
        "\n",
        "    def _ruleset_to_df(self):\n",
        "        longest_rule_len = max([len(r) for r in self.ruleset_.rules])\n",
        "        rows = []\n",
        "        for r in self.ruleset_.rules:\n",
        "            if len(r) > longest_rule_len:\n",
        "                longest_rule_len = len(r)\n",
        "            new_row = [str(c) for c in r.conds]\n",
        "            new_row.extend([None] * (longest_rule_len - len(new_row)))\n",
        "            rows.append(new_row)\n",
        "\n",
        "        df = pd.DataFrame().from_records(data=rows)\n",
        "        return df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "epdpYFyzYRw4"
      },
      "source": [
        "#CatNap.py\n",
        "class CatNap:\n",
        "   \n",
        "    def __init__(\n",
        "        self,\n",
        "        df_or_arr,\n",
        "        columns=None,\n",
        "        feat_subset=None,\n",
        "        cond_subset=None,\n",
        "        class_feat=None,\n",
        "        pos_class=None,\n",
        "    ):\n",
        "        df = pd.DataFrame(df_or_arr)\n",
        "\n",
        "        if columns:\n",
        "            df.columns = columns\n",
        "\n",
        "        if class_feat is None:\n",
        "            self.conds = self.possible_conds(df) if cond_subset is None else cond_subset\n",
        "            self.cond_maps = dict(\n",
        "                [(c, set(c.covers(df).index.tolist())) for c in self.conds]\n",
        "            )\n",
        "\n",
        "        else:\n",
        "            self.conds = (\n",
        "                self.possible_conds(df.drop(class_feat, axis=1))\n",
        "                if cond_subset is None\n",
        "                else [c for c in cond_subset if c.feature != class_feat]\n",
        "            )\n",
        "            self.cond_maps = dict(\n",
        "                [\n",
        "                    (c, set(c.covers(df.drop(class_feat, axis=1)).index.tolist()))\n",
        "                    for c in self.conds\n",
        "                ]\n",
        "            )\n",
        "\n",
        "        self.num_conds = len(self.conds)\n",
        "        self.num_idx = len(df)\n",
        "        self.all = set(df.index.tolist())\n",
        "\n",
        "    def __str__(self):\n",
        "        return (\n",
        "            f\"<CatNap object: {self.num_conds} Conds covering {self.num_idx} examples>\"\n",
        "        )\n",
        "\n",
        "    __repr__ = __str__\n",
        "\n",
        "    def cond_covers(self, cond, subset=None):\n",
        "        return (\n",
        "            self.cond_maps.get(cond)\n",
        "            if subset is None\n",
        "            else self.cond_maps.get(cond).intersection(subset)\n",
        "        )\n",
        "\n",
        "    def rule_covers(self, rule, subset=None):\n",
        "        if rule.conds:\n",
        "            covered = set.intersection(*[self.cond_maps.get(c) for c in rule.conds])\n",
        "            return covered if subset is None else covered.intersection(subset)\n",
        "        else:\n",
        "            return self.all if subset is None else self.all.intersection(subset)\n",
        "\n",
        "    def ruleset_covers(self, ruleset, subset=None):\n",
        "        allpos, allneg = ruleset._check_allpos_allneg(warn=False)\n",
        "        if allpos:\n",
        "            return self.all if not subset else subset\n",
        "        elif allneg:\n",
        "            return set()\n",
        "        else:\n",
        "            return (\n",
        "                set.union(\n",
        "                    *[\n",
        "                        set.intersection(*[self.cond_maps.get(c) for c in r.conds])\n",
        "                        for r in ruleset\n",
        "                    ]\n",
        "                )\n",
        "                if subset is None\n",
        "                else set.union(\n",
        "                    *[\n",
        "                        set.intersection(*[self.cond_maps.get(c) for c in r.conds])\n",
        "                        for r in ruleset\n",
        "                    ]\n",
        "                ).intersection(subset)\n",
        "            )\n",
        "\n",
        "    def to_df(self, coverage):\n",
        "        return df.loc[sorted(list(coverage))]\n",
        "\n",
        "    def possible_conds(self, df):\n",
        "        conds = []\n",
        "        for feat in df.columns.values:\n",
        "            for val in df[feat].unique():\n",
        "                conds.append(Cond(feat, val))\n",
        "        return conds\n",
        "\n",
        "    def pos_idx_neg_idx(\n",
        "        self, df=None, class_feat=None, pos_class=None, pos_df=None, neg_df=None\n",
        "    ):\n",
        "        \"\"\"Pass in df, pos_class, and class_feat or pos_df and neg_df.\"\"\"\n",
        "        if pos_df is None and neg_df is None:\n",
        "            pos_df = df[df[class_feat] == pos_class]\n",
        "            neg_df = df[df[class_feat] != pos_class]\n",
        "\n",
        "        pos_idx = set(pos_df.index.tolist())\n",
        "        neg_idx = set(neg_df.index.tolist())\n",
        "\n",
        "        return pos_idx, neg_idx"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3E8wQIHUYcBh"
      },
      "source": [
        "#check.py\n",
        "\n",
        "def _warn(message, category, filename, funcname, warnstack=[]):\n",
        "    \"\"\"Prettier version of warnings warnings.\n",
        "    warnstack: (optional) list of tuples of filename and function(s) calling the function where warning occurs.\n",
        "    \"\"\"\n",
        "    message = \"\\n\" + message + \"\\n\"\n",
        "    filename += \".py\"\n",
        "    funcname = \" .\" + funcname\n",
        "    if warnstack:\n",
        "        filename = (\n",
        "            str(\n",
        "                [\n",
        "                    stack_filename + \".py: .\" + stack_funcname + \" | \"\n",
        "                    for stack_filename, stack_funcname in warnstack\n",
        "                ]\n",
        "            )\n",
        "            .strip(\"[\")\n",
        "            .strip(\"]\")\n",
        "            .strip(\" \")\n",
        "            .strip(\"'\")\n",
        "            + filename\n",
        "        )\n",
        "    warnings.showwarning(message, category, filename=filename, lineno=funcname)\n",
        "\n",
        "\n",
        "def _check_any_datasets_not_empty(datasets):\n",
        "    return any([len(dataset) > 0 for dataset in datasets])\n",
        "\n",
        "\n",
        "def _check_is_model_fit(model):\n",
        "    if not hasattr(model, \"ruleset_\"):\n",
        "        raise AttributeError(\n",
        "            \"You should fit the ruleset classifier with .fit method before making predictions with it.\"\n",
        "        )\n",
        "\n",
        "\n",
        "# TODO: Check in fit methods before fitting\n",
        "def _check_any_pos(df, class_feat, pos_class):\n",
        "    pass\n",
        "\n",
        "\n",
        "def _check_any_neg(df, class_feat, pos_class):\n",
        "    pass\n",
        "\n",
        "\n",
        "def _check_all_of_type(iterable, type_):\n",
        "    wrong_type_objects = []\n",
        "    for object in iterable:\n",
        "        if not isinstance(object, type_):\n",
        "            wrong_type_objects.append(object)\n",
        "    if wrong_type_objects:\n",
        "        wrong_info = [(object, type(object).__name__) for object in wrong_type_objects]\n",
        "        raise TypeError(f\"Objects must be of type {type_}: {wrong_info}\")\n",
        "\n",
        "\n",
        "def _check_param_deprecation(kwargs, parameters):\n",
        "    passed_parameters = []\n",
        "    for param in kwargs.keys():\n",
        "        if param in parameters:\n",
        "            passed_parameters.append(param)\n",
        "    if passed_parameters:\n",
        "        _warn(\n",
        "            f\".fit: In the future, define parameters: {passed_parameters} during IREP/RIPPER object initialization instead of during model fitting.\",\n",
        "            DeprecationWarning,\n",
        "            \"irep/ripper\",\n",
        "            \"fit\",\n",
        "        )\n",
        "\n",
        "\n",
        "def _check_model_features_present(df, model_selected_features):\n",
        "\n",
        "    df_feats = df.columns.tolist()\n",
        "    missing_feats = [f for f in model_selected_features if f not in df_feats]\n",
        "    if missing_feats:\n",
        "        raise IndexError(\n",
        "            f\"The features selected by Ruleset model need to be present in prediction dataset. Dataset includes: {df_feats}.\\nMissing features names: {missing_feats}.\\nEither ensure prediction dataset includes all Ruleset-selected features with same names as training set, or use parameter 'feature_names' to specify the names of prediction dataset features.\\n\"\n",
        "        )\n",
        "\n",
        "\n",
        "def _warn_only_single_class(only_value, pos_class, filename, funcname):\n",
        "    missing_class = \"positive\" if only_value != pos_class else \"negative\"\n",
        "    warning_str = f\"No {missing_class} samples. All target labels={only_value}.\"\n",
        "    _warn(\n",
        "        warning_str, RuntimeWarning, filename=filename, funcname=funcname,\n",
        "    )\n",
        "\n",
        "\n",
        "def _check_valid_index(index, iterable, source_func):\n",
        "    if index < 0 or index >= len(iterable):\n",
        "        raise IndexError(\n",
        "            f\"{source_func}: {index} is out of range; {iterable} is of length {len(iterable)}\"\n",
        "        )\n",
        "\n",
        "\n",
        "def _check_rule_exists(rule, ruleset, source_func):\n",
        "    for r in ruleset:\n",
        "        if r == rule:\n",
        "            return\n",
        "    raise ValueError(\n",
        "        f\"{source_func}: couldn't find Rule named '{rule}' in Ruleset: '{ruleset}'\"\n",
        "    )"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dC8R8KB8Yldq"
      },
      "source": [
        "#utils.py\n",
        "\n",
        "\n",
        "def drop_chars(str_, chars):\n",
        "    res = str_\n",
        "    for char in chars:\n",
        "        res = res.replace(char, \"\")\n",
        "    return res\n",
        "\n",
        "\n",
        "def remove_duplicates(list_):\n",
        "    res = deepcopy(list_)\n",
        "    encountered = set()\n",
        "    i = 0\n",
        "    while i < len(res):\n",
        "        if res[i] in encountered:\n",
        "            del(res[i])\n",
        "        else:\n",
        "            encountered.add(res[i])\n",
        "            i += 1\n",
        "    return res\n",
        "\n",
        "\n",
        "def aslist(data):\n",
        "    try:\n",
        "        return data.aslist()\n",
        "    except:\n",
        "        return data\n",
        "\n",
        "\n",
        "def try_np_tonum(value):\n",
        "    try:\n",
        "        return value.item()\n",
        "    except:\n",
        "        return value\n",
        "\n",
        "\n",
        "def flagged_return(flags, objects):\n",
        "    \"\"\"Return only objects with corresponding True flags. Useful for functions with multiple possible return items.\"\"\"\n",
        "    if sum(flags) == 1:\n",
        "        return objects[0]\n",
        "    elif sum(flags) > 1:\n",
        "        return tuple([object for flag, object in zip(flags, objects) if flag])\n",
        "    else:\n",
        "        return ()\n",
        "\n",
        "\n",
        "def rnd(float, places=None):\n",
        "    \"\"\"Round a float to decimal places.\n",
        "    float : float\n",
        "        Value to round.\n",
        "    places : int, default=None\n",
        "        Number of decimal places to round to. None defaults to 1 decimal place if float < 100, otherwise defaults to 0 places.\n",
        "    \"\"\"\n",
        "    if places is None:\n",
        "        if float < 1:\n",
        "            places = 2\n",
        "        elif float < 100:\n",
        "            places = 1\n",
        "        else:\n",
        "            places = 0\n",
        "    rounded = round(float, places)\n",
        "    if rounded != int(rounded):\n",
        "        return rounded\n",
        "    else:\n",
        "        return int(rounded)\n",
        "\n",
        "\n",
        "def weighted_avg_freqs(counts):\n",
        "    \"\"\"Return weighted mean proportions of counts in the list.\n",
        "    counts <list<tuple>>\n",
        "    \"\"\"\n",
        "    arr = np.array(counts)\n",
        "    total = arr.flatten().sum()\n",
        "    return arr.sum(axis=0) / total if total else arr.sum(axis=0)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qQFt4ENxViIv"
      },
      "source": [
        "#ripper.py\n",
        "class RIPPER(AbstractRulesetClassifier):\n",
        "    def __init__(\n",
        "        self,\n",
        "        k=2,\n",
        "        dl_allowance=64,\n",
        "        prune_size=0.33,\n",
        "        n_discretize_bins=10,\n",
        "        max_rules=None,\n",
        "        max_rule_conds=None,\n",
        "        max_total_conds=None,\n",
        "        random_state=None,\n",
        "        verbosity=0,\n",
        "    ):\n",
        "       \n",
        "\n",
        "        AbstractRulesetClassifier.__init__(\n",
        "            self,\n",
        "            algorithm_name=\"RIPPER\",\n",
        "            prune_size=prune_size,\n",
        "            n_discretize_bins=n_discretize_bins,\n",
        "            max_rules=max_rules,\n",
        "            max_rule_conds=max_rule_conds,\n",
        "            max_total_conds=max_total_conds,\n",
        "            random_state=random_state,\n",
        "            verbosity=verbosity,\n",
        "        )\n",
        "        self.VALID_HYPERPARAMETERS.update({\"k\", \"dl_allowance\"})\n",
        "        self.k = k\n",
        "        self.dl_allowance = dl_allowance\n",
        "\n",
        "    def __str__(self):\n",
        "        \"\"\"Return string representation of a RIPPER classifier.\"\"\"\n",
        "        params = str(self.get_params()) + \">\"\n",
        "        params = (\n",
        "            params.replace(\": \", \"=\")\n",
        "            .replace(\"'\", \"\")\n",
        "            .replace(\"{\", \"(\")\n",
        "            .replace(\"}\", \")\")\n",
        "        )\n",
        "        return f\"<RIPPER{params}\"\n",
        "\n",
        "    def out_model(self):\n",
        "        \"\"\"Print trained Ruleset model line-by-line: V represents 'or'; ^ represents 'and'.\"\"\"\n",
        "        super().out_model()\n",
        "\n",
        "    def fit(\n",
        "        self,\n",
        "        trainset,\n",
        "        y=None,\n",
        "        class_feat=None,\n",
        "        pos_class=None,\n",
        "        feature_names=None,\n",
        "        initial_model=None,\n",
        "        cn_optimize=True,\n",
        "        **kwargs,\n",
        "    ):\n",
        "        \n",
        "\n",
        "        ################\n",
        "        # Stage 0: Setup\n",
        "        ################\n",
        "\n",
        "        # Handle any hyperparam deprecation\n",
        "        self._set_deprecated_fit_params(kwargs)\n",
        "\n",
        "        # Preprocess training data\n",
        "        preprocess_params = {\n",
        "            \"trainset\": trainset,\n",
        "            \"y\": y,\n",
        "            \"class_feat\": class_feat,\n",
        "            \"pos_class\": pos_class,\n",
        "            \"feature_names\": feature_names,\n",
        "            \"n_discretize_bins\": self.n_discretize_bins,\n",
        "            \"verbosity\": self.verbosity,\n",
        "        }\n",
        "        (\n",
        "            df,\n",
        "            self.class_feat,\n",
        "            self.pos_class,\n",
        "            self.bin_transformer_,\n",
        "        ) = preprocess_training_data(preprocess_params)\n",
        "\n",
        "        # Create CatNap\n",
        "        # possible minor speedup if pass cond_subset of only pos_class conds?\n",
        "        if cn_optimize:\n",
        "            self.cn = CatNap(\n",
        "                df,\n",
        "                feat_subset=None,\n",
        "                cond_subset=None,\n",
        "                class_feat=self.class_feat,\n",
        "                pos_class=None,\n",
        "            )\n",
        "\n",
        "        # Split df into pos, neg classes\n",
        "        pos_df, neg_df = pos_neg_split(\n",
        "            df, self.class_feat, self.pos_class\n",
        "        )\n",
        "        pos_df = pos_df.drop(self.class_feat, axis=1)\n",
        "        neg_df = neg_df.drop(self.class_feat, axis=1)\n",
        "\n",
        "        # Collect possible conds\n",
        "        self._set_possible_conds(df)\n",
        "\n",
        "        ###############################\n",
        "        # Stage 1: Grow initial Ruleset\n",
        "        ###############################\n",
        "\n",
        "        if cn_optimize:\n",
        "            pos_idx = set(pos_df.index.tolist())\n",
        "            neg_idx = set(neg_df.index.tolist())\n",
        "            self.ruleset_ = self._grow_ruleset_cn(\n",
        "                pos_idx,\n",
        "                neg_idx,\n",
        "                prune_size=self.prune_size,\n",
        "                dl_allowance=self.dl_allowance,\n",
        "                max_rules=self.max_rules,\n",
        "                max_rule_conds=self.max_rule_conds,\n",
        "                max_total_conds=self.max_total_conds,\n",
        "                initial_model=initial_model,\n",
        "                random_state=self.random_state,\n",
        "            )\n",
        "        else:\n",
        "            self.ruleset_ = self._grow_ruleset(\n",
        "                pos_df,\n",
        "                neg_df,\n",
        "                prune_size=self.prune_size,\n",
        "                dl_allowance=self.dl_allowance,\n",
        "                max_rules=self.max_rules,\n",
        "                max_rule_conds=self.max_rule_conds,\n",
        "                max_total_conds=self.max_total_conds,\n",
        "                initial_model=initial_model,\n",
        "                random_state=self.random_state,\n",
        "            )\n",
        "        if self.verbosity >= 1:\n",
        "            print()\n",
        "            print(\"GREW INITIAL RULESET:\")\n",
        "            self.ruleset_.out_pretty()\n",
        "            print()\n",
        "\n",
        "        ###########################\n",
        "        # Stage 2: Optimize Ruleset\n",
        "        ###########################\n",
        "\n",
        "        for iter in range(1, self.k + 1):\n",
        "            # Create new but reproducible random_state (if applicable)\n",
        "            iter_random_state = (\n",
        "                self.random_state + 100 if self.random_state is not None else None\n",
        "            )\n",
        "            # Run optimization iteration\n",
        "            if self.verbosity >= 1:\n",
        "                print(f\"optimization run {iter} of {self.k}\")\n",
        "            if cn_optimize:\n",
        "                newset = self._optimize_ruleset_cn(\n",
        "                    self.ruleset_,\n",
        "                    pos_idx,\n",
        "                    neg_idx,\n",
        "                    prune_size=self.prune_size,\n",
        "                    random_state=iter_random_state,\n",
        "                )\n",
        "            else:\n",
        "                newset = self._optimize_ruleset(\n",
        "                    self.ruleset_,\n",
        "                    pos_df,\n",
        "                    neg_df,\n",
        "                    prune_size=self.prune_size,\n",
        "                    random_state=iter_random_state,\n",
        "                )\n",
        "\n",
        "            if self.verbosity >= 1:\n",
        "                print()\n",
        "                print(\"OPTIMIZED RULESET:\")\n",
        "                if self.verbosity >= 2:\n",
        "                    print(\n",
        "                        f\"iteration {iter} of {self.k}\\n modified rules {[i for i in range(len(self.ruleset_.rules)) if self.ruleset_.rules[i]!= newset.rules[i]]}\"\n",
        "                    )\n",
        "                newset.out_pretty()\n",
        "                print()\n",
        "\n",
        "            if iter != self.k and self.ruleset_ == newset:\n",
        "                if self.verbosity >= 1:\n",
        "                    print(\"No changes were made. Halting optimization.\")\n",
        "                break\n",
        "            else:\n",
        "                self.ruleset_ = newset\n",
        "\n",
        "        #############################################\n",
        "        # Stage 3: Cover any last remaining positives\n",
        "        #############################################\n",
        "\n",
        "        if cn_optimize:\n",
        "            self._cover_remaining_positives_cn(\n",
        "                df,\n",
        "                max_rules=self.max_rules,\n",
        "                max_rule_conds=self.max_rule_conds,\n",
        "                max_total_conds=self.max_total_conds,\n",
        "                random_state=self.random_state,\n",
        "            )\n",
        "        else:\n",
        "            self._cover_remaining_positives(\n",
        "                df,\n",
        "                max_rules=self.max_rules,\n",
        "                max_rule_conds=self.max_rule_conds,\n",
        "                max_total_conds=self.max_total_conds,\n",
        "                random_state=self.random_state,\n",
        "            )\n",
        "\n",
        "        #################################################\n",
        "        # Stage 4: Remove any rules that don't improve dl\n",
        "        #################################################\n",
        "\n",
        "        if self.verbosity >= 2:\n",
        "            print(\"Optimizing dl...\")\n",
        "        if cn_optimize:\n",
        "            mdl_subset, _ = _rs_total_bits_cn(\n",
        "                self.cn,\n",
        "                self.ruleset_,\n",
        "                self.ruleset_.possible_conds,\n",
        "                pos_idx,\n",
        "                neg_idx,\n",
        "                bestsubset_dl=True,\n",
        "                ret_bestsubset=True,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "        else:\n",
        "            mdl_subset, _ = _rs_total_bits(\n",
        "                self.ruleset_,\n",
        "                self.ruleset_.possible_conds,\n",
        "                pos_df,\n",
        "                neg_df,\n",
        "                bestsubset_dl=True,\n",
        "                ret_bestsubset=True,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "        self.ruleset_ = mdl_subset\n",
        "        if self.verbosity >= 1:\n",
        "            print(\"FINAL RULESET:\")\n",
        "            self.ruleset_.out_pretty()\n",
        "            print()\n",
        "\n",
        "        # Issue warning if Ruleset is universal or empty\n",
        "        self.ruleset_._check_allpos_allneg(warn=True, warnstack=[(\"ripper\", \"fit\")])\n",
        "\n",
        "        # Set Ruleset features\n",
        "        self.selected_features_ = self.ruleset_.get_selected_features()\n",
        "        self.trainset_features_ = df.drop(self.class_feat, axis=1).columns.tolist()\n",
        "\n",
        "        # Remove any duplicates and trim\n",
        "        self.ruleset_.rules = remove_duplicates(self.ruleset_.rules)\n",
        "        self.ruleset_.trim_conds(max_total_conds=self.max_total_conds)\n",
        "\n",
        "        # Fit probas\n",
        "        self.recalibrate_proba(\n",
        "            df, min_samples=None, require_min_samples=False, discretize=False\n",
        "        )\n",
        "        self.classes_ = np.array([0, 1])\n",
        "\n",
        "        # Cleanup\n",
        "        if cn_optimize:\n",
        "            del self.cn\n",
        "\n",
        "    \n",
        "    def score(self, X, y, score_function=score_accuracy):\n",
        "        _check_is_model_fit(self)\n",
        "\n",
        "        predictions = self.predict(X)\n",
        "        actuals = [yi == self.pos_class for yi in aslist(y)]\n",
        "        return score_function(actuals, predictions)\n",
        "\n",
        "    def _set_theory_dl_lookup(self, df, size=15, verbosity=0):\n",
        "        \"\"\"Precalculate rule theory dls for various-sized rules.\"\"\"\n",
        "\n",
        "        self.dl_dict = {}\n",
        "\n",
        "        temp = Ruleset()\n",
        "        temp._set_possible_conds(df, df)\n",
        "\n",
        "        for n in range(1, size + 1):\n",
        "            rule = Rule([Cond(\"_\", \"_\")] * n)\n",
        "            dl = _r_theory_bits(\n",
        "                rule, temp.possible_conds, bits_dict=None, verbosity=verbosity\n",
        "            )\n",
        "            self.dl_dict[n] = dl\n",
        "            if verbosity >= 2:\n",
        "                print(f\"updated dl for rule size {n}: {dl}\")\n",
        "\n",
        "    def _grow_ruleset(\n",
        "        self,\n",
        "        pos_df,\n",
        "        neg_df,\n",
        "        prune_size,\n",
        "        dl_allowance,\n",
        "        max_rules=None,\n",
        "        max_rule_conds=None,\n",
        "        max_total_conds=None,\n",
        "        initial_model=None,\n",
        "        random_state=None,\n",
        "    ):\n",
        "        \"\"\"Grow a Ruleset with pruning.\"\"\"\n",
        "        ruleset = self._ruleset_frommodel(initial_model)\n",
        "        ruleset._set_possible_conds(pos_df, neg_df)\n",
        "\n",
        "        ruleset_dl = None\n",
        "        mdl = None  # Minimum encountered description length (in bits)\n",
        "        dl_diff = 0\n",
        "        if self.verbosity >= 2:\n",
        "            print(\"growing ruleset...\")\n",
        "            print(f\"initial model: {ruleset}\")\n",
        "            print()\n",
        "\n",
        "        pos_remaining = pos_df.copy()\n",
        "        neg_remaining = neg_df.copy()\n",
        "        while len(pos_remaining) > 0 and dl_diff <= self.dl_allowance:\n",
        "\n",
        "            # If applicable, check for user-specified early stopping\n",
        "            if stop_early(ruleset, max_rules, max_total_conds):\n",
        "                break\n",
        "\n",
        "            # Grow-prune split remaining uncovered examples\n",
        "            pos_growset, pos_pruneset = df_shuffled_split(\n",
        "                pos_remaining, (1 - prune_size), random_state=random_state\n",
        "            )\n",
        "            neg_growset, neg_pruneset = df_shuffled_split(\n",
        "                neg_remaining, (1 - prune_size), random_state=random_state\n",
        "            )\n",
        "            if self.verbosity >= 2:\n",
        "                print(\n",
        "                    f\"pos_growset {len(pos_growset)} pos_pruneset {len(pos_pruneset)}\"\n",
        "                )\n",
        "                print(\n",
        "                    f\"neg_growset {len(neg_growset)} neg_pruneset {len(neg_pruneset)}\"\n",
        "                )\n",
        "            if len(pos_growset) == 0:\n",
        "                break  # Probably safe, but a little dicey to only check pos_growset.\n",
        "\n",
        "            # Grow Rule\n",
        "            grown_rule = grow_rule(\n",
        "                pos_growset,\n",
        "                neg_growset,\n",
        "                ruleset.possible_conds,\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            if grown_rule.isempty():\n",
        "                break  # Generated an empty rule b/c no good conds exist\n",
        "\n",
        "            # Prune Rule\n",
        "            pruned_rule = prune_rule(\n",
        "                grown_rule,\n",
        "                _RIPPER_growphase_prune_metric,\n",
        "                pos_pruneset,\n",
        "                neg_pruneset,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "\n",
        "            # Add rule; calculate new description length\n",
        "            ruleset.add(\n",
        "                pruned_rule\n",
        "            )  # Unlike IREP, IREP*/RIPPER stopping condition is inclusive: \"After each rule is added, the total description length of the rule set and examples is computed.\"\n",
        "            if self.verbosity >= 2:\n",
        "                print(f\"updated ruleset: {ruleset.truncstr(direction='right')}\")\n",
        "                print()\n",
        "\n",
        "            if ruleset_dl is None:  # First Rule to be added\n",
        "                rule_dl = _r_theory_bits(\n",
        "                    pruned_rule, ruleset.possible_conds, verbosity=self.verbosity\n",
        "                )\n",
        "                theory_dl = rule_dl\n",
        "                data_dl = _exceptions_bits(\n",
        "                    ruleset, pos_df, neg_df, verbosity=self.verbosity\n",
        "                )\n",
        "                ruleset_dl = theory_dl + data_dl\n",
        "                mdl = ruleset_dl\n",
        "            else:\n",
        "                rule_dl = _r_theory_bits(\n",
        "                    pruned_rule, ruleset.possible_conds, verbosity=self.verbosity\n",
        "                )\n",
        "                theory_dl += rule_dl\n",
        "                data_dl = _exceptions_bits(\n",
        "                    ruleset, pos_df, neg_df, verbosity=self.verbosity\n",
        "                )\n",
        "                ruleset_dl = theory_dl + data_dl\n",
        "                dl_diff = ruleset_dl - mdl\n",
        "\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"rule dl: {rnd(rule_dl)}\")\n",
        "                print(f\"updated theory dl: {rnd(theory_dl)}\")\n",
        "                print(f\"exceptions: {rnd(data_dl)}\")\n",
        "                print(f\"total dl: {rnd(ruleset_dl)}\")\n",
        "                if dl_diff <= self.dl_allowance:\n",
        "                    print(\n",
        "                        f\"mdl {rnd(mdl)} (diff {rnd(dl_diff)} <= {rnd(self.dl_allowance)})\"\n",
        "                    )\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"mdl {rnd(mdl)} dl-halt: diff {rnd(dl_diff)} exceeds allowance ({rnd(self.dl_allowance)})\"\n",
        "                    )\n",
        "\n",
        "            mdl = ruleset_dl if ruleset_dl < mdl else mdl\n",
        "\n",
        "            # Remove covered examples\n",
        "            pos_remaining, neg_remaining = rm_covered(\n",
        "                pruned_rule, pos_remaining, neg_remaining\n",
        "            )\n",
        "\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"examples remaining: {len(pos_remaining)} pos, {len(neg_remaining)} neg\"\n",
        "                )\n",
        "                print()\n",
        "\n",
        "        return ruleset\n",
        "\n",
        "    def _grow_ruleset_cn(\n",
        "        self,\n",
        "        pos_idx,\n",
        "        neg_idx,\n",
        "        prune_size,\n",
        "        dl_allowance,\n",
        "        max_rules=None,\n",
        "        max_rule_conds=None,\n",
        "        max_total_conds=None,\n",
        "        initial_model=None,\n",
        "        random_state=None,\n",
        "    ):\n",
        "        \"\"\"Grow a Ruleset with pruning.\"\"\"\n",
        "        ruleset = self._ruleset_frommodel(initial_model)\n",
        "        ruleset.possible_conds = self.cn.conds\n",
        "\n",
        "        pos_remaining_idx = pos_idx\n",
        "        neg_remaining_idx = neg_idx\n",
        "        ruleset_dl = None\n",
        "        mdl = None  # Minimum encountered description length (in bits)\n",
        "        dl_diff = 0\n",
        "        if self.verbosity >= 2:\n",
        "            print(\"growing ruleset...\")\n",
        "            print(f\"initial model: {ruleset}\")\n",
        "            print()\n",
        "\n",
        "        while len(pos_remaining_idx) > 0 and dl_diff <= self.dl_allowance:\n",
        "\n",
        "            # If applicable, check for user-specified early stopping\n",
        "            if (max_rules is not None and len(ruleset.rules) >= max_rules) or (\n",
        "                max_total_conds is not None and ruleset.count_conds() >= max_total_conds\n",
        "            ):\n",
        "                break\n",
        "\n",
        "            # Grow-prune split remaining uncovered examples\n",
        "            pos_growset_idx, pos_pruneset_idx = random_split(\n",
        "                pos_remaining_idx,\n",
        "                (1 - prune_size),\n",
        "                res_type=set,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "            neg_growset_idx, neg_pruneset_idx = random_split(\n",
        "                neg_remaining_idx,\n",
        "                (1 - prune_size),\n",
        "                res_type=set,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "            if self.verbosity >= 2:\n",
        "                print(\n",
        "                    f\"pos_growset {len(pos_growset_idx)} pos_pruneset {len(pos_pruneset_idx)}\"\n",
        "                )\n",
        "                print(\n",
        "                    f\"neg_growset {len(neg_growset_idx)} neg_pruneset {len(neg_pruneset_idx)}\"\n",
        "                )\n",
        "            if len(pos_growset_idx) == 0:\n",
        "                break  # Probably safe, but a little dicey to only check pos_growset.\n",
        "\n",
        "            # Grow Rule\n",
        "            grown_rule = grow_rule_cn(\n",
        "                self.cn,\n",
        "                pos_growset_idx,\n",
        "                neg_growset_idx,\n",
        "                initial_rule=Rule(),\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            if grown_rule.isempty():\n",
        "                break  # Generated an empty rule b/c no good conds exist\n",
        "\n",
        "            # Prune Rule\n",
        "            pruned_rule = prune_rule_cn(\n",
        "                self.cn,\n",
        "                grown_rule,\n",
        "                _RIPPER_growphase_prune_metric_cn,\n",
        "                pos_pruneset_idx,\n",
        "                neg_pruneset_idx,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "\n",
        "            # Add rule; calculate new description length\n",
        "            ruleset.add(\n",
        "                pruned_rule\n",
        "            )  # Unlike IREP, IREP*/RIPPER stopping condition is inclusive: \"After each rule is added, the total description length of the rule set and examples is computed.\"\n",
        "            if self.verbosity >= 2:\n",
        "                print(f\"updated ruleset: {ruleset.truncstr(direction='right')}\")\n",
        "                print()\n",
        "\n",
        "            if ruleset_dl is None:  # First Rule to be added\n",
        "                rule_dl = _r_theory_bits(\n",
        "                    pruned_rule, ruleset.possible_conds, verbosity=self.verbosity\n",
        "                )\n",
        "                theory_dl = rule_dl\n",
        "                data_dl = _exceptions_bits_cn(\n",
        "                    self.cn, ruleset, pos_idx, neg_idx, verbosity=self.verbosity\n",
        "                )\n",
        "                ruleset_dl = theory_dl + data_dl\n",
        "                mdl = ruleset_dl\n",
        "            else:\n",
        "                rule_dl = _r_theory_bits(\n",
        "                    pruned_rule, ruleset.possible_conds, verbosity=self.verbosity\n",
        "                )\n",
        "                theory_dl += rule_dl\n",
        "                data_dl = _exceptions_bits_cn(\n",
        "                    self.cn, ruleset, pos_idx, neg_idx, verbosity=self.verbosity\n",
        "                )\n",
        "                ruleset_dl = theory_dl + data_dl\n",
        "                dl_diff = ruleset_dl - mdl\n",
        "\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"rule dl: {rnd(rule_dl)}\")\n",
        "                print(f\"updated theory dl: {rnd(theory_dl)}\")\n",
        "                print(f\"exceptions: {rnd(data_dl)}\")\n",
        "                print(f\"total dl: {rnd(ruleset_dl)}\")\n",
        "                if dl_diff <= self.dl_allowance:\n",
        "                    print(\n",
        "                        f\"mdl {rnd(mdl)} (diff {rnd(dl_diff)} <= {rnd(self.dl_allowance)})\"\n",
        "                    )\n",
        "                else:\n",
        "                    print(\n",
        "                        f\"mdl {rnd(mdl)} dl-halt: diff {rnd(dl_diff)} exceeds allowance ({rnd(self.dl_allowance)})\"\n",
        "                    )\n",
        "\n",
        "            mdl = ruleset_dl if ruleset_dl < mdl else mdl\n",
        "\n",
        "            # Remove covered examples\n",
        "            pos_remaining_idx, neg_remaining_idx = rm_rule_covers_cn(\n",
        "                self.cn, pruned_rule, pos_remaining_idx, neg_remaining_idx\n",
        "            )\n",
        "\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"examples remaining: {len(pos_remaining_idx)} pos, {len(neg_remaining_idx)} neg\"\n",
        "                )\n",
        "                print()\n",
        "\n",
        "        return ruleset\n",
        "\n",
        "    def _optimize_ruleset(\n",
        "        self,\n",
        "        ruleset,\n",
        "        pos_df,\n",
        "        neg_df,\n",
        "        prune_size,\n",
        "        max_rule_conds=None,\n",
        "        random_state=None,\n",
        "    ):\n",
        "        \"\"\"Optimization phase.\"\"\"\n",
        "\n",
        "        if self.verbosity >= 2:\n",
        "            print(\"optimizing ruleset...\")\n",
        "            print()\n",
        "\n",
        "        pos_remaining = pos_df.copy()\n",
        "        neg_remaining = neg_df.copy()\n",
        "        original_ruleset = copy.deepcopy(ruleset)\n",
        "        if self.verbosity >= 4:\n",
        "            print(\"calculate original ruleset potential dl...\")\n",
        "        original_dl = _rs_total_bits(\n",
        "            original_ruleset,\n",
        "            original_ruleset.possible_conds,\n",
        "            pos_df,\n",
        "            neg_df,\n",
        "            bestsubset_dl=True,\n",
        "            verbosity=self.verbosity,\n",
        "        )\n",
        "        if self.verbosity >= 3:\n",
        "            print(f\"original ruleset potential dl: {rnd(original_dl)}\")\n",
        "            print()\n",
        "        new_ruleset = copy.deepcopy(ruleset)\n",
        "\n",
        "        for i, rule in enumerate(original_ruleset.rules):\n",
        "            pos_growset, pos_pruneset = df_shuffled_split(\n",
        "                pos_remaining, (1 - prune_size), random_state=random_state\n",
        "            )\n",
        "            neg_growset, neg_pruneset = df_shuffled_split(\n",
        "                neg_remaining, (1 - prune_size), random_state=random_state\n",
        "            )\n",
        "            if len(pos_growset) == 0:\n",
        "                break  # Possible where optimization run > 1\n",
        "\n",
        "            # Create alternative rules\n",
        "            if self.verbosity >= 4:\n",
        "                print(\n",
        "                    f\"creating replacement for {i} of {len(original_ruleset.rules)}: {ruleset.rules[i]}\"\n",
        "                )\n",
        "            g_replacement = grow_rule(\n",
        "                pos_growset,\n",
        "                neg_growset,\n",
        "                original_ruleset.possible_conds,\n",
        "                initial_rule=Rule(),\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            replacement_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, g_replacement)\n",
        "            )\n",
        "            pr_replacement = prune_rule(\n",
        "                g_replacement,\n",
        "                _RIPPER_optimization_prune_metric,\n",
        "                pos_pruneset,\n",
        "                neg_pruneset,\n",
        "                eval_index_on_ruleset=(i, replacement_ruleset),\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            replacement_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, pr_replacement)\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"grew replacement {g_replacement}\")\n",
        "                print(f\"pruned replacement is {pr_replacement}\")\n",
        "\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"creating revision for {i} of {len(original_ruleset.rules)}: {ruleset.rules[i]}\"\n",
        "                )\n",
        "            g_revision = grow_rule(\n",
        "                pos_growset,\n",
        "                neg_growset,\n",
        "                original_ruleset.possible_conds,\n",
        "                initial_rule=ruleset.rules[i],\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            revision_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, g_revision)\n",
        "            )\n",
        "            pr_revision = prune_rule(\n",
        "                g_revision,\n",
        "                _RIPPER_optimization_prune_metric,\n",
        "                pos_pruneset,\n",
        "                neg_pruneset,\n",
        "                eval_index_on_ruleset=(i, revision_ruleset),\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            revision_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, pr_revision)\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"grew revision {g_replacement}\")\n",
        "                print(f\"pruned revision is {pr_replacement}\")\n",
        "                print()\n",
        "\n",
        "            # Calculate alternative Rulesets' respective lowest potential dls to identify the best version\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"calculate potential dl for ds with replacement {pr_replacement}\"\n",
        "                )\n",
        "            replacement_dl = (\n",
        "                _rs_total_bits(\n",
        "                    replacement_ruleset,\n",
        "                    original_ruleset.possible_conds,\n",
        "                    pos_df,\n",
        "                    neg_df,\n",
        "                    bestsubset_dl=True,\n",
        "                    verbosity=self.verbosity,\n",
        "                )\n",
        "                if pr_replacement != rule\n",
        "                else original_dl\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"calculate potential dl for ds with revision {pr_revision}\")\n",
        "            revision_dl = (\n",
        "                _rs_total_bits(\n",
        "                    revision_ruleset,\n",
        "                    original_ruleset.possible_conds,\n",
        "                    pos_df,\n",
        "                    neg_df,\n",
        "                    bestsubset_dl=True,\n",
        "                    verbosity=self.verbosity,\n",
        "                )\n",
        "                if pr_revision != rule\n",
        "                else original_dl\n",
        "            )\n",
        "            best_rule = [rule, pr_replacement, pr_revision][\n",
        "                argmin([original_dl, replacement_dl, revision_dl])\n",
        "            ]\n",
        "\n",
        "            if self.verbosity >= 2:\n",
        "                print(f\"\\nrule {i+1} of {len(original_ruleset.rules)}\")\n",
        "                rep_str = (\n",
        "                    pr_replacement.__str__() if pr_replacement != rule else \"unchanged\"\n",
        "                )\n",
        "                rev_str = pr_revision.__str__() if pr_revision != rule else \"unchanged\"\n",
        "                best_str = best_rule.__str__() if best_rule != rule else \"unchanged\"\n",
        "                if self.verbosity == 2:\n",
        "                    print(f\"original: {rule}\")\n",
        "                    print(f\"replacement: {rep_str}\")\n",
        "                    print(f\"revision: {rev_str}\")\n",
        "                    print(f\"*best: {best_str}\")\n",
        "                    if best_rule in new_ruleset:\n",
        "                        print(\n",
        "                            f\"best already included in optimization -- retaining original\"\n",
        "                        )\n",
        "                    print()\n",
        "                else:\n",
        "                    print(f\"original: {rule}) | {rnd(original_dl)} bits\")\n",
        "                    print(f\"replacement: {rep_str} | {rnd(replacement_dl)} bits\")\n",
        "                    print(f\"revision: {rev_str} | {rnd(revision_dl)} bits\")\n",
        "                    print(\n",
        "                        f\"*best: {best_str} | {rnd(min([replacement_dl, revision_dl, original_dl]))} bits\"\n",
        "                    )\n",
        "                    if best_rule in new_ruleset:\n",
        "                        print(\n",
        "                            f\"best already included in optimization -- retaining original\"\n",
        "                        )\n",
        "                    print()\n",
        "            if best_rule not in new_ruleset:\n",
        "                new_ruleset.rules[i] = best_rule\n",
        "            else:\n",
        "                new_ruleset.rules[i] = rule\n",
        "\n",
        "            # Remove covered examples\n",
        "            pos_remaining, neg_remaining = rm_covered(\n",
        "                rule, pos_remaining, neg_remaining\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"examples remaining: {len(pos_remaining)} pos, {len(neg_remaining)} neg\"\n",
        "                )\n",
        "                print()\n",
        "\n",
        "            # If there are no pos data remaining to train optimization (could happen if optimization run >1), keep remaining rules the same\n",
        "            if len(pos_remaining) == 0:\n",
        "                break\n",
        "\n",
        "        return new_ruleset\n",
        "\n",
        "    def _optimize_ruleset_cn(\n",
        "        self,\n",
        "        ruleset,\n",
        "        pos_idx,\n",
        "        neg_idx,\n",
        "        prune_size,\n",
        "        max_rule_conds=None,\n",
        "        random_state=None,\n",
        "    ):\n",
        "        \"\"\"Optimization phase.\"\"\"\n",
        "\n",
        "        if self.verbosity >= 2:\n",
        "            print(\"optimizing ruleset...\")\n",
        "            print()\n",
        "\n",
        "        pos_remaining_idx = pos_idx\n",
        "        neg_remaining_idx = neg_idx\n",
        "        original_ruleset = copy.deepcopy(ruleset)\n",
        "        if self.verbosity >= 4:\n",
        "            print(\"calculate original ruleset potential dl...\")\n",
        "        original_dl = _rs_total_bits_cn(\n",
        "            self.cn,\n",
        "            original_ruleset,\n",
        "            original_ruleset.possible_conds,\n",
        "            pos_idx,\n",
        "            neg_idx,\n",
        "            bestsubset_dl=True,\n",
        "            verbosity=self.verbosity,\n",
        "        )\n",
        "        if self.verbosity >= 3:\n",
        "            print(f\"original ruleset potential dl: {rnd(original_dl)}\")\n",
        "            print()\n",
        "        new_ruleset = copy.deepcopy(ruleset)\n",
        "\n",
        "        for i, rule in enumerate(original_ruleset.rules):\n",
        "            pos_growset_idx, pos_pruneset_idx = set_shuffled_split(\n",
        "                pos_remaining_idx, (1 - prune_size), random_state=random_state\n",
        "            )\n",
        "            neg_growset_idx, neg_pruneset_idx = set_shuffled_split(\n",
        "                neg_remaining_idx, (1 - prune_size), random_state=random_state\n",
        "            )\n",
        "            if len(pos_growset_idx) == 0:\n",
        "                break  # Possible where optimization run > 1\n",
        "\n",
        "            # Create alternative rules\n",
        "            if self.verbosity >= 4:\n",
        "                print(\n",
        "                    f\"creating replacement for {i} of {len(original_ruleset.rules)}: {ruleset.rules[i]}\"\n",
        "                )\n",
        "            # g_replacement = grow_rule(pos_growset, neg_growset, original_ruleset.possible_conds, initial_rule=Rule(), max_rule_conds=max_rule_conds, verbosity=self.verbosity)\n",
        "            g_replacement = grow_rule_cn(\n",
        "                self.cn,\n",
        "                pos_growset_idx,\n",
        "                neg_growset_idx,\n",
        "                initial_rule=Rule(),\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            replacement_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, g_replacement)\n",
        "            )\n",
        "            # pr_replacement = prune_rule(g_replacement, _RIPPER_optimization_prune_metric, pos_pruneset, neg_pruneset, eval_index_on_ruleset=(i,replacement_ruleset), verbosity=self.verbosity)\n",
        "            pr_replacement = prune_rule_cn(\n",
        "                self.cn,\n",
        "                g_replacement,\n",
        "                _RIPPER_optimization_prune_metric_cn,\n",
        "                pos_pruneset_idx,\n",
        "                neg_pruneset_idx,\n",
        "                eval_index_on_ruleset=(i, replacement_ruleset),\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            replacement_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, pr_replacement)\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"grew replacement {g_replacement}\")\n",
        "                print(f\"pruned replacement is {pr_replacement}\")\n",
        "\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"creating revision for {i} of {len(original_ruleset.rules)}: {ruleset.rules[i]}\"\n",
        "                )\n",
        "            # g_revision = grow_rule(pos_growset, neg_growset, original_ruleset.possible_conds, initial_rule=ruleset.rules[i], max_rule_conds=max_rule_conds, verbosity=self.verbosity)\n",
        "            g_revision = grow_rule_cn(\n",
        "                self.cn,\n",
        "                pos_growset_idx,\n",
        "                neg_growset_idx,\n",
        "                initial_rule=ruleset.rules[i],\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            revision_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, g_revision)\n",
        "            )\n",
        "            # pr_revision = prune_rule(g_revision, _RIPPER_optimization_prune_metric, pos_pruneset, neg_pruneset, eval_index_on_ruleset=(i,revision_ruleset), verbosity=self.verbosity)\n",
        "            pr_revision = prune_rule_cn(\n",
        "                self.cn,\n",
        "                g_revision,\n",
        "                _RIPPER_optimization_prune_metric_cn,\n",
        "                pos_pruneset_idx,\n",
        "                neg_pruneset_idx,\n",
        "                eval_index_on_ruleset=(i, revision_ruleset),\n",
        "                verbosity=self.verbosity,\n",
        "            )\n",
        "            revision_ruleset = Ruleset(\n",
        "                i_replaced(original_ruleset.rules, i, pr_revision)\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"grew revision {g_replacement}\")\n",
        "                print(f\"pruned revision is {pr_replacement}\")\n",
        "                print()\n",
        "\n",
        "            # Calculate alternative Rulesets' respective lowest potential dls to identify the best version\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"calculate potential dl for ds with replacement {pr_replacement}\"\n",
        "                )\n",
        "            replacement_dl = (\n",
        "                _rs_total_bits_cn(\n",
        "                    self.cn,\n",
        "                    replacement_ruleset,\n",
        "                    original_ruleset.possible_conds,\n",
        "                    pos_idx,\n",
        "                    neg_idx,\n",
        "                    bestsubset_dl=False,\n",
        "                    ret_bestsubset=False,\n",
        "                    verbosity=0,\n",
        "                )\n",
        "                if pr_replacement != rule\n",
        "                else original_dl\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(f\"calculate potential dl for ds with revision {pr_revision}\")\n",
        "            revision_dl = (\n",
        "                _rs_total_bits_cn(\n",
        "                    self.cn,\n",
        "                    revision_ruleset,\n",
        "                    original_ruleset.possible_conds,\n",
        "                    pos_idx,\n",
        "                    neg_idx,\n",
        "                    bestsubset_dl=False,\n",
        "                    ret_bestsubset=False,\n",
        "                    verbosity=0,\n",
        "                )\n",
        "                if pr_revision != rule\n",
        "                else original_dl\n",
        "            )\n",
        "            best_rule = [rule, pr_replacement, pr_revision][\n",
        "                argmin([original_dl, replacement_dl, revision_dl])\n",
        "            ]\n",
        "\n",
        "            if self.verbosity >= 2:\n",
        "                print(f\"\\nrule {i+1} of {len(original_ruleset.rules)}\")\n",
        "                rep_str = (\n",
        "                    pr_replacement.__str__() if pr_replacement != rule else \"unchanged\"\n",
        "                )\n",
        "                rev_str = pr_revision.__str__() if pr_revision != rule else \"unchanged\"\n",
        "                best_str = best_rule.__str__() if best_rule != rule else \"unchanged\"\n",
        "                if self.verbosity == 2:\n",
        "                    print(f\"original: {rule}\")\n",
        "                    print(f\"replacement: {rep_str}\")\n",
        "                    print(f\"revision: {rev_str}\")\n",
        "                    print(f\"*best: {best_str}\")\n",
        "                    if best_rule in new_ruleset:\n",
        "                        print(\n",
        "                            f\"best already included in optimization -- retaining original\"\n",
        "                        )\n",
        "                    print()\n",
        "                else:\n",
        "                    print(f\"original: {rule}) | {rnd(original_dl)} bits\")\n",
        "                    print(f\"replacement: {rep_str} | {rnd(replacement_dl)} bits\")\n",
        "                    print(f\"revision: {rev_str} | {rnd(revision_dl)} bits\")\n",
        "                    print(\n",
        "                        f\"*best: {best_str} | {rnd(min([replacement_dl, revision_dl, original_dl]))} bits\"\n",
        "                    )\n",
        "                    if best_rule in new_ruleset:\n",
        "                        print(\n",
        "                            f\"best already included in optimization -- retaining original\"\n",
        "                        )\n",
        "                    print()\n",
        "            if best_rule not in new_ruleset:\n",
        "                new_ruleset.rules[i] = best_rule\n",
        "            else:\n",
        "                new_ruleset.rules[i] = rule\n",
        "\n",
        "            # Remove covered examples\n",
        "            pos_remaining_idx, neg_remaining_idx = rm_rule_covers_cn(\n",
        "                self.cn, rule, pos_remaining_idx, neg_remaining_idx\n",
        "            )\n",
        "            if self.verbosity >= 3:\n",
        "                print(\n",
        "                    f\"examples remaining: {len(pos_remaining_idx)} pos, {len(neg_remaining_idx)} neg\"\n",
        "                )\n",
        "                print()\n",
        "\n",
        "            # If there are no pos data remaining to train optimization (could happen if optimization run >1), keep remaining rules the same\n",
        "            if len(pos_remaining_idx) == 0:\n",
        "                break\n",
        "\n",
        "        return new_ruleset\n",
        "\n",
        "    def _set_possible_conds(self, df):\n",
        "        self.possible_conds = []\n",
        "        for feat in df.columns.values:\n",
        "            if feat != self.class_feat:\n",
        "                for val in df[feat].unique():\n",
        "                    self.possible_conds.append(Cond(feat, val))\n",
        "\n",
        "    def _cover_remaining_positives(\n",
        "        self,\n",
        "        df,\n",
        "        max_rules=None,\n",
        "        max_rule_conds=None,\n",
        "        max_total_conds=None,\n",
        "        random_state=None,\n",
        "    ):\n",
        "        \"\"\"Stage 3: Post-optimization, cover any remaining uncovered positives.\"\"\"\n",
        "        pos_remaining, neg_remaining = pos_neg_split(\n",
        "            df, self.class_feat, self.pos_class\n",
        "        )\n",
        "        pos_remaining = pos_remaining.drop(self.class_feat, axis=1)\n",
        "        neg_remaining = neg_remaining.drop(self.class_feat, axis=1)\n",
        "        pos_remaining, neg_remaining = rm_covered(\n",
        "            self.ruleset_, pos_remaining, neg_remaining\n",
        "        )\n",
        "        if len(pos_remaining) >= 1:\n",
        "            if self.verbosity >= 2:\n",
        "                print(f\"{len(pos_remaining)} pos left. Growing final rules...\")\n",
        "            newset = self._grow_ruleset(\n",
        "                pos_remaining,\n",
        "                neg_remaining,\n",
        "                initial_model=self.ruleset_,\n",
        "                prune_size=self.prune_size,\n",
        "                dl_allowance    =self.dl_allowance,\n",
        "                max_rules=max_rules,\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                max_total_conds=max_total_conds,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "            if self.verbosity >= 1:\n",
        "                print(\"GREW FINAL RULES\")\n",
        "                newset.out_pretty()\n",
        "                print()\n",
        "            self.ruleset_ = newset\n",
        "        else:\n",
        "            if self.verbosity >= 1:\n",
        "                print(\"All pos covered\\n\")\n",
        "\n",
        "    def _cover_remaining_positives_cn(\n",
        "        self,\n",
        "        df,\n",
        "        max_rules=None,\n",
        "        max_rule_conds=None,\n",
        "        max_total_conds=None,\n",
        "        random_state=None,\n",
        "    ):\n",
        "        \"\"\"Stage 3: Post-optimization, cover any remaining uncovered positives.\"\"\"\n",
        "        pos_remaining_idx, neg_remaining_idx = self.cn.pos_idx_neg_idx(\n",
        "            df, self.class_feat, self.pos_class\n",
        "        )\n",
        "\n",
        "        if len(pos_remaining_idx) >= 1:\n",
        "            if self.verbosity >= 2:\n",
        "                print(f\"{len(pos_remaining_idx)} pos left. Growing final rules...\")\n",
        "            newset = self._grow_ruleset_cn(\n",
        "                pos_remaining_idx,\n",
        "                neg_remaining_idx,\n",
        "                initial_model=self.ruleset_,\n",
        "                prune_size=self.prune_size,\n",
        "                dl_allowance=self.dl_allowance,\n",
        "                max_rules=max_rules,\n",
        "                max_rule_conds=max_rule_conds,\n",
        "                max_total_conds=max_total_conds,\n",
        "                random_state=random_state,\n",
        "            )\n",
        "            if self.verbosity >= 1:\n",
        "                print(\"GREW FINAL RULES\")\n",
        "                newset.out_pretty()\n",
        "                print()\n",
        "            self.ruleset_ = newset\n",
        "        else:\n",
        "            if self.verbosity >= 1:\n",
        "                print(\"All positives covered\\n\")\n",
        "\n",
        "    def get_params(self, deep=True):\n",
        "        return {param: self.__dict__.get(param) for param in self.VALID_HYPERPARAMETERS}\n",
        "\n",
        "    def set_params(self, **parameters):\n",
        "        for parameter, value in parameters.items():\n",
        "            # if parameter in self.VALID_HYPERPARAMETERS:\n",
        "            setattr(self, parameter, value)\n",
        "        return self\n",
        "\n",
        "\n",
        "###################################\n",
        "##### RIPPER-specific Metrics #####\n",
        "###################################\n",
        "\n",
        "\n",
        "def _RIPPER_growphase_prune_metric(rule, pos_pruneset, neg_pruneset):\n",
        "\n",
        "    # I imagine Weka's is 1/2 because that's closer to a 50-50 class distribution?\n",
        "    p = rule.num_covered(pos_pruneset)\n",
        "    n = rule.num_covered(neg_pruneset)\n",
        "    return (p - n + 1) / (p + n + 1)\n",
        "\n",
        "\n",
        "def _RIPPER_growphase_prune_metric_cn(cn, rule, pos_pruneset_idx, neg_pruneset_idx):\n",
        " \n",
        "    # I imagine Weka's is 1/2 because that's closer to a 50-50 class distribution?\n",
        "    p = len(cn.rule_covers(rule, pos_pruneset_idx))\n",
        "    n = len(cn.rule_covers(rule, neg_pruneset_idx))\n",
        "    return (p - n + 1) / (p + n + 1)\n",
        "\n",
        "\n",
        "def _RIPPER_optimization_prune_metric(rule, pos_pruneset, neg_pruneset):\n",
        "    return _accuracy(rule, pos_pruneset, neg_pruneset)\n",
        "\n",
        "\n",
        "def _RIPPER_optimization_prune_metric_cn(cn, rule, pos_pruneset_idx, neg_pruneset_idx):\n",
        "    return _rule_accuracy_cn(\n",
        "        cn, rule, pos_pruneset_idx, neg_pruneset_idx\n",
        "    )\n",
        "\n",
        "\n",
        "def _r_theory_bits(rule, possible_conds, bits_dict=None, verbosity=0):\n",
        "    \"\"\"Returns description length (in bits) for a single Rule.\"\"\"\n",
        "\n",
        "    if hasattr(rule, \"dl\"):\n",
        "        return rule.dl\n",
        "    else:\n",
        "        # if type(rule) != Rule:\n",
        "        #    raise TypeError(f'param rule in _r_theory_bits is type {type(rule)}; it should be type Rule')\n",
        "        k = len(rule.conds)  # Number of rule conditions\n",
        "        n = len(possible_conds)  # Number of possible conditions\n",
        "        pr = k / n\n",
        "\n",
        "        S = k * math.log2(1 / pr) + (n - k) * math.log2((1 / (1 - pr)))  # S(n, k, pr)\n",
        "        K = math.log2(k)  # Number bits need to send integer k\n",
        "        rule_dl = 0.5 * (\n",
        "            K + S\n",
        "        )  # Divide by 2 a la Quinlan. Cohen: \"to adjust for possible redundency in attributes\"\n",
        "        if verbosity >= 5:\n",
        "            print(\n",
        "                f\"rule theory bits| {rule} k {k} n {n} pr {rnd(pr)}: {rnd(rule_dl)} bits\"\n",
        "            )\n",
        "\n",
        "        # rule.dl = rule_dl\n",
        "        return rule_dl\n",
        "\n",
        "\n",
        "def _rs_theory_bits(ruleset, possible_conds, verbosity=0):\n",
        "    \"\"\"Returns theory description length (in bits) for a Ruleset.\"\"\"\n",
        "\n",
        "    # if type(ruleset) != Ruleset:\n",
        "    #    raise TypeError(f'param ruleset in _rs_theory_bits should be type Ruleset')\n",
        "    total = 0\n",
        "    for rule in ruleset.rules:\n",
        "        total += _r_theory_bits(rule, possible_conds, verbosity=verbosity)\n",
        "        # total += rule_bits(rule, possible_conds, rem_pos, rem_neg, verbosity=verbosity)\n",
        "        # rem_pos, rem_neg = base.rm_covered(rule, rem_pos, rem_neg)\n",
        "    if verbosity >= 5:\n",
        "        print(f\"ruleset theory bits| {rnd(total)}\")\n",
        "\n",
        "    # ruleset.dl = total\n",
        "    return total\n",
        "\n",
        "\n",
        "def _exceptions_bits(ruleset, pos_df, neg_df, verbosity=0):\n",
        "    \"\"\"Returns description length (in bits) for exceptions to a Ruleset's coverage.\"\"\"\n",
        "\n",
        "    if type(ruleset) != Ruleset:\n",
        "        raise TypeError(\n",
        "            f\"to avoid double-counting, _exceptions_bits should calculate exceptions over entire set of rules with type Ruleset\"\n",
        "        )\n",
        "    N = len(pos_df) + len(neg_df)  # Total number of examples\n",
        "    p = ruleset.num_covered(pos_df) + ruleset.num_covered(\n",
        "        neg_df\n",
        "    )  # Total number of examples classified as positive = total covered\n",
        "    fp = ruleset.num_covered(\n",
        "        neg_df\n",
        "    )  # Number false positives = negatives covered by the ruleset\n",
        "    fn = len(pos_df) - ruleset.num_covered(\n",
        "        pos_df\n",
        "    )  # Number false negatives = positives not covered by the ruleset\n",
        "    exceptions_dl = math.log2(nCr(p, fp)) + math.log2(nCr((N - p), fn))\n",
        "    if verbosity >= 5:\n",
        "        print(\n",
        "            f\"exceptions_bits| {ruleset.truncstr()}: \\n N {N} p {p} fp {fp} fn {fn}: exceptions_bits {rnd(exceptions_dl)}\"\n",
        "        )\n",
        "\n",
        "    return exceptions_dl\n",
        "\n",
        "\n",
        "def _exceptions_bits_cn(cn, ruleset, pos_idx, neg_idx, verbosity=0):\n",
        "    \"\"\"Returns description length (in bits) for exceptions to a Ruleset's coverage.\"\"\"\n",
        "\n",
        "    # if type(ruleset) != Ruleset:\n",
        "    #    raise TypeError(f'to avoid double-counting, _exceptions_bits should calculate exceptions over entire set of rules with type Ruleset')\n",
        "    N = len(pos_idx) + len(neg_idx)  # Total number of examples\n",
        "    pos_cov = cn.ruleset_covers(ruleset, subset=pos_idx)\n",
        "    neg_cov = cn.ruleset_covers(ruleset, subset=neg_idx)\n",
        "    p = len(pos_cov) + len(\n",
        "        neg_cov\n",
        "    )  # Total number of examples classified as positive = total covered\n",
        "    fp = len(neg_cov)  # Number false positives = negatives covered by the ruleset\n",
        "    fn = len(pos_idx) - len(\n",
        "        pos_cov\n",
        "    )  # Number false negatives = positives not covered by the ruleset\n",
        "    exceptions_dl = math.log2(nCr(p, fp)) + math.log2(nCr((N - p), fn))\n",
        "    if verbosity >= 5:\n",
        "        print(\n",
        "            f\"exceptions_bits| {ruleset.truncstr()}: \\n N {N} p {p} fp {fp} fn {fn}: exceptions_bits {rnd(exceptions_dl)}\"\n",
        "        )\n",
        "\n",
        "    return exceptions_dl\n",
        "\n",
        "\n",
        "def _rs_total_bits(\n",
        "    ruleset,\n",
        "    possible_conds,\n",
        "    pos_df,\n",
        "    neg_df,\n",
        "    bestsubset_dl=False,\n",
        "    ret_bestsubset=False,\n",
        "    verbosity=0,\n",
        "):\n",
        "    \n",
        "    if ret_bestsubset and not bestsubset_dl:\n",
        "        raise ValueError(\n",
        "            f\"ret_bestsubset must be True in order to return bestsubset_dl\"\n",
        "        )\n",
        "\n",
        "    if not bestsubset_dl:\n",
        "        theory_bits = _rs_theory_bits(ruleset, possible_conds, verbosity=verbosity)\n",
        "        data_bits = _exceptions_bits(ruleset, pos_df, neg_df, verbosity=verbosity)\n",
        "        if verbosity >= 3:\n",
        "            print(f\"total ruleset bits | {rnd(theory_bits + data_bits)}\")\n",
        "        return theory_bits + data_bits\n",
        "    else:\n",
        "        # Collect the dl of each subset\n",
        "        subset_dls = []\n",
        "        theory_dl = 0\n",
        "        if verbosity >= 5:\n",
        "            print(f\"find best potential dl for {ruleset}:\")\n",
        "        for i, rule in enumerate(\n",
        "            ruleset.rules\n",
        "        ):  # Separating theory and exceptions dls in this way means you don't have to recalculate theory each time\n",
        "            subset = Ruleset(ruleset.rules[: i + 1])\n",
        "            rule_theory_dl = _r_theory_bits(rule, possible_conds, verbosity=verbosity)\n",
        "            theory_dl += rule_theory_dl\n",
        "            exceptions_dl = _exceptions_bits(\n",
        "                subset, pos_df, neg_df, verbosity=verbosity\n",
        "            )\n",
        "            subset_dls.append(theory_dl + exceptions_dl)\n",
        "            if verbosity >= 5:\n",
        "                print(f\"subset 0-{i} | dl: {rnd(subset_dls[i])}\")\n",
        "\n",
        "        # Build up the best Ruleset and calculate the mdl\n",
        "        mdl_ruleset = Ruleset()\n",
        "        for i, rule, in enumerate(ruleset.rules):\n",
        "            if (\n",
        "                i == 0 or subset_dls[i] <= subset_dls[i - 1]\n",
        "            ):  # Rule i does not worsen the dl\n",
        "                mdl_ruleset.add(rule)\n",
        "        if verbosity >= 5:\n",
        "            print(f\"subset dls: {[(i,rnd(dl)) for i,dl in enumerate(subset_dls)]}\")\n",
        "            print(f\"best potential ruleset: {mdl_ruleset}\")\n",
        "        mdl = _rs_total_bits(\n",
        "            mdl_ruleset,\n",
        "            possible_conds,\n",
        "            pos_df,\n",
        "            neg_df,\n",
        "            bestsubset_dl=False,\n",
        "            verbosity=0,\n",
        "        )  # About to print value below\n",
        "        if verbosity >= 5:\n",
        "            print(f\"best potential dl was {rnd(mdl)}\")\n",
        "            print()\n",
        "        if not ret_bestsubset:\n",
        "            return mdl\n",
        "        else:\n",
        "            return (mdl_ruleset, mdl)\n",
        "\n",
        "\n",
        "def _rs_total_bits_cn(\n",
        "    cn,\n",
        "    ruleset,\n",
        "    possible_conds,\n",
        "    pos_idx,\n",
        "    neg_idx,\n",
        "    bestsubset_dl=False,\n",
        "    ret_bestsubset=False,\n",
        "    verbosity=0,\n",
        "):\n",
        "    \n",
        "\n",
        "    if ret_bestsubset and not bestsubset_dl:\n",
        "        raise ValueError(\n",
        "            f\"ret_bestsubset must be True in order to return bestsubset_dl\"\n",
        "        )\n",
        "\n",
        "    if not bestsubset_dl:\n",
        "        theory_bits = _rs_theory_bits(ruleset, possible_conds, verbosity=verbosity)\n",
        "        data_bits = _exceptions_bits_cn(\n",
        "            cn, ruleset, pos_idx, neg_idx, verbosity=verbosity\n",
        "        )\n",
        "        if verbosity >= 3:\n",
        "            print(f\"total ruleset bits | {rnd(theory_bits + data_bits)}\")\n",
        "        return theory_bits + data_bits\n",
        "    else:\n",
        "        # Collect the dl of each subset\n",
        "        subset_dls = []\n",
        "        theory_dl = 0\n",
        "        if verbosity >= 5:\n",
        "            print(f\"find best potential dl for {ruleset}:\")\n",
        "        for i, rule in enumerate(\n",
        "            ruleset.rules\n",
        "        ):  # Separating theory and exceptions dls in this way means you don't have to recalculate theory each time\n",
        "            subset = Ruleset(ruleset.rules[: i + 1])\n",
        "            rule_theory_dl = _r_theory_bits(rule, possible_conds, verbosity=verbosity)\n",
        "            theory_dl += rule_theory_dl\n",
        "            exceptions_dl = _exceptions_bits_cn(\n",
        "                cn, subset, pos_idx, neg_idx, verbosity=verbosity\n",
        "            )\n",
        "            subset_dls.append(theory_dl + exceptions_dl)\n",
        "            if verbosity >= 5:\n",
        "                print(f\"subset 0-{i} | dl: {rnd(subset_dls[i])}\")\n",
        "\n",
        "        # Build up the best Ruleset and calculate the mdl\n",
        "        mdl_ruleset = Ruleset()\n",
        "        for i, rule, in enumerate(ruleset.rules):\n",
        "            if (\n",
        "                i == 0 or subset_dls[i] <= subset_dls[i - 1]\n",
        "            ):  # Rule i does not worsen the dl\n",
        "                mdl_ruleset.add(rule)\n",
        "        if verbosity >= 5:\n",
        "            print(f\"subset dls: {[(i,rnd(dl)) for i,dl in enumerate(subset_dls)]}\")\n",
        "            print(f\"best potential ruleset: {mdl_ruleset}\")\n",
        "        mdl = _rs_total_bits_cn(\n",
        "            cn,\n",
        "            mdl_ruleset,\n",
        "            possible_conds,\n",
        "            pos_idx,\n",
        "            neg_idx,\n",
        "            bestsubset_dl=False,\n",
        "            verbosity=0,\n",
        "        )  # About to print value below\n",
        "        if verbosity >= 5:\n",
        "            print(f\"best potential dl was {rnd(mdl)}\")\n",
        "            print()\n",
        "        if not ret_bestsubset:\n",
        "            return mdl\n",
        "        else:\n",
        "            return (mdl_ruleset, mdl)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MTfs7D38kM52",
        "outputId": "cd03c0b0-ee28-46b0-e9d6-935ffdd17ea9"
      },
      "source": [
        "ripper_clf = RIPPER(random_state=42, verbosity=2)\n",
        "ripper_clf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<RIPPER(prune_size=0.33, random_state=42, k=2, n_discretize_bins=10, verbosity=2, max_total_conds=None, max_rules=None, max_rule_conds=None, dl_allowance=64)>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "paX2xUx6kZnq",
        "outputId": "fea79e75-2e25-4f44-e82d-ce610ceb641a"
      },
      "source": [
        "ripper_clf.fit(train, class_feat='Category', pos_class=5)\n",
        "ripper_clf.ruleset_ "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fitting bins for features ['Age', 'ALB', 'ALP', 'ALT', 'AST', 'BIL', 'CHE', 'CHOL', 'CREA', 'GGT', 'PROT']\n",
            "\n",
            "growing ruleset...\n",
            "initial model: []\n",
            "\n",
            "pos_growset 10 pos_pruneset 6\n",
            "neg_growset 265 neg_pruneset 131\n",
            "grew rule: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "pruned rule unchanged\n",
            "updated ruleset: [[AST=95.2-319.8^CHOL=1.43-4.02]]\n",
            "\n",
            "pos_growset 4 pos_pruneset 3\n",
            "neg_growset 264 neg_pruneset 131\n",
            "grew rule: [CHE=1.42-5.7^ALP=100.4-416.6]\n",
            "pruned rule unchanged\n",
            "updated ruleset: [[AST=95.2-319.8^CHOL=1.43-4.02] V [CHE=1.42-5.7^ALP=100.4-416.6]]\n",
            "\n",
            "pos_growset 2 pos_pruneset 2\n",
            "neg_growset 263 neg_pruneset 131\n",
            "grew rule: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5^Sex=1]\n",
            "pruned rule: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5]\n",
            "updated ruleset: ...[[CHE=1.42-5.7^ALP=100.4-416.6] V [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5]]\n",
            "\n",
            "pos_growset 1 pos_pruneset 1\n",
            "neg_growset 263 neg_pruneset 130\n",
            "grew rule: [PROT=80.6-86.5^ALB=38.1-39.7]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5] V [PROT=80.6-86.5^ALB=38.1-39.7]]\n",
            "\n",
            "pos_growset 0 pos_pruneset 1\n",
            "neg_growset 262 neg_pruneset 130\n",
            "\n",
            "GREW INITIAL RULESET:\n",
            "[[AST=95.2-319.8 ^ CHOL=1.43-4.02] V\n",
            "[CHE=1.42-5.7 ^ ALP=100.4-416.6] V\n",
            "[CHOL=1.43-4.02 ^ ALT=0.9-11.9 ^ ALB=14.9-35.5] V\n",
            "[PROT=80.6-86.5 ^ ALB=38.1-39.7]]\n",
            "\n",
            "optimization run 1 of 2\n",
            "optimizing ruleset...\n",
            "\n",
            "grew rule: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "grew rule: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "\n",
            "rule 1 of 4\n",
            "original: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "replacement: unchanged\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "grew rule: [GGT=92.3-650.9^Age=57.0-64.0]\n",
            "grew rule: [CHE=1.42-5.7^ALP=100.4-416.6]\n",
            "\n",
            "rule 2 of 4\n",
            "original: [CHE=1.42-5.7^ALP=100.4-416.6]\n",
            "replacement: [GGT=92.3-650.9^Age=57.0-64.0]\n",
            "revision: unchanged\n",
            "*best: [GGT=92.3-650.9^Age=57.0-64.0]\n",
            "\n",
            "grew rule: [CHOL=1.43-4.02^ALB=14.9-35.5^ALT=0.9-11.9^Sex=1]\n",
            "grew rule: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5^Sex=1]\n",
            "\n",
            "rule 3 of 4\n",
            "original: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5]\n",
            "replacement: unchanged\n",
            "revision: [CHOL=1.43-4.02^ALT=0.9-11.9]\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "grew rule: [AST=95.2-319.8^Sex=1]\n",
            "grew rule: [PROT=80.6-86.5^ALB=38.1-39.7]\n",
            "\n",
            "rule 4 of 4\n",
            "original: [PROT=80.6-86.5^ALB=38.1-39.7]\n",
            "replacement: [AST=95.2-319.8^Sex=1]\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "\n",
            "OPTIMIZED RULESET:\n",
            "iteration 1 of 2\n",
            " modified rules [1]\n",
            "[[AST=95.2-319.8 ^ CHOL=1.43-4.02] V\n",
            "[GGT=92.3-650.9 ^ Age=57.0-64.0] V\n",
            "[CHOL=1.43-4.02 ^ ALT=0.9-11.9 ^ ALB=14.9-35.5] V\n",
            "[PROT=80.6-86.5 ^ ALB=38.1-39.7]]\n",
            "\n",
            "optimization run 2 of 2\n",
            "optimizing ruleset...\n",
            "\n",
            "grew rule: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "grew rule: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "\n",
            "rule 1 of 4\n",
            "original: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "replacement: unchanged\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "grew rule: [GGT=92.3-650.9^Age=57.0-64.0]\n",
            "grew rule: [GGT=92.3-650.9^Age=57.0-64.0]\n",
            "\n",
            "rule 2 of 4\n",
            "original: [GGT=92.3-650.9^Age=57.0-64.0]\n",
            "replacement: unchanged\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "grew rule: [CHOL=1.43-4.02^ALB=14.9-35.5^ALT=0.9-11.9^Sex=1]\n",
            "grew rule: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5^Sex=1]\n",
            "\n",
            "rule 3 of 4\n",
            "original: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5]\n",
            "replacement: unchanged\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "\n",
            "OPTIMIZED RULESET:\n",
            "iteration 2 of 2\n",
            " modified rules []\n",
            "[[AST=95.2-319.8 ^ CHOL=1.43-4.02] V\n",
            "[GGT=92.3-650.9 ^ Age=57.0-64.0] V\n",
            "[CHOL=1.43-4.02 ^ ALT=0.9-11.9 ^ ALB=14.9-35.5] V\n",
            "[PROT=80.6-86.5 ^ ALB=38.1-39.7]]\n",
            "\n",
            "16 pos left. Growing final rules...\n",
            "growing ruleset...\n",
            "initial model: [[AST=95.2-319.8^CHOL=1.43-4.02] V [GGT=92.3-650.9^Age=57.0-64.0] V [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5] V [PROT=80.6-86.5^ALB=38.1-39.7]]\n",
            "\n",
            "pos_growset 10 pos_pruneset 6\n",
            "neg_growset 265 neg_pruneset 131\n",
            "grew rule: [AST=95.2-319.8^CHOL=1.43-4.02]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[PROT=80.6-86.5^ALB=38.1-39.7] V [AST=95.2-319.8^CHOL=1.43-4.02]]\n",
            "\n",
            "pos_growset 4 pos_pruneset 3\n",
            "neg_growset 264 neg_pruneset 131\n",
            "grew rule: [CHE=1.42-5.7^ALP=100.4-416.6]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[AST=95.2-319.8^CHOL=1.43-4.02] V [CHE=1.42-5.7^ALP=100.4-416.6]]\n",
            "\n",
            "pos_growset 2 pos_pruneset 2\n",
            "neg_growset 263 neg_pruneset 131\n",
            "grew rule: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5^Sex=1]\n",
            "pruned rule: [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5]\n",
            "updated ruleset: ...[[CHE=1.42-5.7^ALP=100.4-416.6] V [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5]]\n",
            "\n",
            "pos_growset 1 pos_pruneset 1\n",
            "neg_growset 263 neg_pruneset 130\n",
            "grew rule: [PROT=80.6-86.5^ALB=38.1-39.7]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5] V [PROT=80.6-86.5^ALB=38.1-39.7]]\n",
            "\n",
            "pos_growset 0 pos_pruneset 1\n",
            "neg_growset 262 neg_pruneset 130\n",
            "GREW FINAL RULES\n",
            "[[AST=95.2-319.8 ^ CHOL=1.43-4.02] V\n",
            "[GGT=92.3-650.9 ^ Age=57.0-64.0] V\n",
            "[CHOL=1.43-4.02 ^ ALT=0.9-11.9 ^ ALB=14.9-35.5] V\n",
            "[PROT=80.6-86.5 ^ ALB=38.1-39.7] V\n",
            "[AST=95.2-319.8 ^ CHOL=1.43-4.02] V\n",
            "[CHE=1.42-5.7 ^ ALP=100.4-416.6] V\n",
            "[CHOL=1.43-4.02 ^ ALT=0.9-11.9 ^ ALB=14.9-35.5] V\n",
            "[PROT=80.6-86.5 ^ ALB=38.1-39.7]]\n",
            "\n",
            "Optimizing dl...\n",
            "FINAL RULESET:\n",
            "[[AST=95.2-319.8 ^ CHOL=1.43-4.02] V\n",
            "[GGT=92.3-650.9 ^ Age=57.0-64.0] V\n",
            "[CHOL=1.43-4.02 ^ ALT=0.9-11.9 ^ ALB=14.9-35.5]]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Ruleset [[AST=95.2-319.8^CHOL=1.43-4.02] V [GGT=92.3-650.9^Age=57.0-64.0] V [CHOL=1.43-4.02^ALT=0.9-11.9^ALB=14.9-35.5]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AK0Pf3FO60D6",
        "outputId": "50e00213-fe3f-40e8-e723-7e120d7d0802"
      },
      "source": [
        "X_train, y_train = train.drop('Category', axis=1), train['Category']\n",
        "X_array, y_array = X_train.values, y_train.values\n",
        "ripper_clf.fit(X_array, y_array, pos_class=5)\n",
        "ripper_clf.ruleset_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "fitting bins for features [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11]\n",
            "\n",
            "growing ruleset...\n",
            "initial model: []\n",
            "\n",
            "pos_growset 10 pos_pruneset 6\n",
            "neg_growset 265 neg_pruneset 131\n",
            "grew rule: [8=1.43-4.02^4=0.9-11.9^2=14.9-35.5]\n",
            "pruned rule unchanged\n",
            "updated ruleset: [[8=1.43-4.02^4=0.9-11.9^2=14.9-35.5]]\n",
            "\n",
            "pos_growset 6 pos_pruneset 3\n",
            "neg_growset 264 neg_pruneset 131\n",
            "grew rule: [5=95.2-319.8^7=1.42-5.7^0=0.0]\n",
            "pruned rule: [5=95.2-319.8^7=1.42-5.7]\n",
            "updated ruleset: [[8=1.43-4.02^4=0.9-11.9^2=14.9-35.5] V [5=95.2-319.8^7=1.42-5.7]]\n",
            "\n",
            "pos_growset 2 pos_pruneset 1\n",
            "neg_growset 263 neg_pruneset 131\n",
            "grew rule: [8=1.43-4.02^2=38.1-39.7]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[5=95.2-319.8^7=1.42-5.7] V [8=1.43-4.02^2=38.1-39.7]]\n",
            "\n",
            "pos_growset 1 pos_pruneset 1\n",
            "neg_growset 263 neg_pruneset 130\n",
            "grew rule: [5=95.2-319.8^0=1.0]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[8=1.43-4.02^2=38.1-39.7] V [5=95.2-319.8^0=1.0]]\n",
            "\n",
            "\n",
            "GREW INITIAL RULESET:\n",
            "[[8=1.43-4.02 ^ 4=0.9-11.9 ^ 2=14.9-35.5] V\n",
            "[5=95.2-319.8 ^ 7=1.42-5.7] V\n",
            "[8=1.43-4.02 ^ 2=38.1-39.7] V\n",
            "[5=95.2-319.8 ^ 0=1.0]]\n",
            "\n",
            "optimization run 1 of 2\n",
            "optimizing ruleset...\n",
            "\n",
            "grew rule: [8=1.43-4.02^4=0.9-11.9^5=95.2-319.8]\n",
            "grew rule: [8=1.43-4.02^4=0.9-11.9^2=14.9-35.5^5=95.2-319.8]\n",
            "\n",
            "rule 1 of 4\n",
            "original: [8=1.43-4.02^4=0.9-11.9^2=14.9-35.5]\n",
            "replacement: [8=1.43-4.02^4=0.9-11.9^5=95.2-319.8]\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "grew rule: [5=95.2-319.8^1=57.0-64.0]\n",
            "grew rule: [5=95.2-319.8^7=1.42-5.7]\n",
            "\n",
            "rule 2 of 4\n",
            "original: [5=95.2-319.8^7=1.42-5.7]\n",
            "replacement: [5=95.2-319.8]\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "grew rule: [8=1.43-4.02^7=5.7-6.71^1=57.0-64.0]\n",
            "grew rule: [8=1.43-4.02^2=38.1-39.7]\n",
            "\n",
            "rule 3 of 4\n",
            "original: [8=1.43-4.02^2=38.1-39.7]\n",
            "replacement: [8=1.43-4.02^7=5.7-6.71^1=57.0-64.0]\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "grew rule: [5=95.2-319.8^0=1.0]\n",
            "grew rule: [5=95.2-319.8^0=1.0]\n",
            "\n",
            "rule 4 of 4\n",
            "original: [5=95.2-319.8^0=1.0]\n",
            "replacement: unchanged\n",
            "revision: unchanged\n",
            "*best: unchanged\n",
            "best already included in optimization -- retaining original\n",
            "\n",
            "\n",
            "OPTIMIZED RULESET:\n",
            "iteration 1 of 2\n",
            " modified rules []\n",
            "[[8=1.43-4.02 ^ 4=0.9-11.9 ^ 2=14.9-35.5] V\n",
            "[5=95.2-319.8 ^ 7=1.42-5.7] V\n",
            "[8=1.43-4.02 ^ 2=38.1-39.7] V\n",
            "[5=95.2-319.8 ^ 0=1.0]]\n",
            "\n",
            "No changes were made. Halting optimization.\n",
            "16 pos left. Growing final rules...\n",
            "growing ruleset...\n",
            "initial model: [[8=1.43-4.02^4=0.9-11.9^2=14.9-35.5] V [5=95.2-319.8^7=1.42-5.7] V [8=1.43-4.02^2=38.1-39.7] V [5=95.2-319.8^0=1.0]]\n",
            "\n",
            "pos_growset 10 pos_pruneset 6\n",
            "neg_growset 265 neg_pruneset 131\n",
            "grew rule: [8=1.43-4.02^4=0.9-11.9^2=14.9-35.5]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[5=95.2-319.8^0=1.0] V [8=1.43-4.02^4=0.9-11.9^2=14.9-35.5]]\n",
            "\n",
            "pos_growset 6 pos_pruneset 3\n",
            "neg_growset 264 neg_pruneset 131\n",
            "grew rule: [5=95.2-319.8^7=1.42-5.7^0=0.0]\n",
            "pruned rule: [5=95.2-319.8^7=1.42-5.7]\n",
            "updated ruleset: ...[[8=1.43-4.02^4=0.9-11.9^2=14.9-35.5] V [5=95.2-319.8^7=1.42-5.7]]\n",
            "\n",
            "pos_growset 2 pos_pruneset 1\n",
            "neg_growset 263 neg_pruneset 131\n",
            "grew rule: [8=1.43-4.02^2=38.1-39.7]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[5=95.2-319.8^7=1.42-5.7] V [8=1.43-4.02^2=38.1-39.7]]\n",
            "\n",
            "pos_growset 1 pos_pruneset 1\n",
            "neg_growset 263 neg_pruneset 130\n",
            "grew rule: [5=95.2-319.8^0=1.0]\n",
            "pruned rule unchanged\n",
            "updated ruleset: ...[[8=1.43-4.02^2=38.1-39.7] V [5=95.2-319.8^0=1.0]]\n",
            "\n",
            "GREW FINAL RULES\n",
            "[[8=1.43-4.02 ^ 4=0.9-11.9 ^ 2=14.9-35.5] V\n",
            "[5=95.2-319.8 ^ 7=1.42-5.7] V\n",
            "[8=1.43-4.02 ^ 2=38.1-39.7] V\n",
            "[5=95.2-319.8 ^ 0=1.0] V\n",
            "[8=1.43-4.02 ^ 4=0.9-11.9 ^ 2=14.9-35.5] V\n",
            "[5=95.2-319.8 ^ 7=1.42-5.7] V\n",
            "[8=1.43-4.02 ^ 2=38.1-39.7] V\n",
            "[5=95.2-319.8 ^ 0=1.0]]\n",
            "\n",
            "Optimizing dl...\n",
            "FINAL RULESET:\n",
            "[[8=1.43-4.02 ^ 4=0.9-11.9 ^ 2=14.9-35.5] V\n",
            "[5=95.2-319.8 ^ 7=1.42-5.7] V\n",
            "[5=95.2-319.8 ^ 0=1.0]]\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Ruleset [[8=1.43-4.02^4=0.9-11.9^2=14.9-35.5] V [5=95.2-319.8^7=1.42-5.7] V [5=95.2-319.8^0=1.0]]>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zbyaotcg9oeD",
        "outputId": "4c45f3f9-dcf5-4fec-9bba-395beaee8d4a"
      },
      "source": [
        "from sklearn.model_selection import cross_val_score\n",
        "\n",
        "# Dummify our data to make sklearn happy\n",
        "X_train = pd.get_dummies(X_train, columns=X_train.select_dtypes('object').columns)\n",
        "y_train = y_train.map(lambda x: 1 if x==5 else 0)\n",
        "\n",
        "ripper_clf = RIPPER(random_state=42)\n",
        "cross_val_score(ripper_clf, X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0.97590361, 0.96385542, 0.95121951, 0.93902439, 0.93902439])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 33
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2oL-xRgt_1h8",
        "outputId": "9c22b177-c270-443a-9369-8250c808ddb0"
      },
      "source": [
        "from sklearn.model_selection import GridSearchCV\n",
        "param_grid = {\"prune_size\": [0.1, 0.25, 0.33, 0.5], \"k\": [1, 2]}\n",
        "grid = GridSearchCV(estimator=ripper_clf, param_grid=param_grid)\n",
        "grid.fit(X_train, y_train)\n",
        "grid.best_params_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'k': 1, 'prune_size': 0.1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LpelKL7WAJYD",
        "outputId": "0e845a99-8c8e-4a27-ab3e-8b499bdd5702"
      },
      "source": [
        "X_test = test.drop('Category', axis=1)\n",
        "y_test = test['Category']\n",
        "ripper_clf = RIPPER(random_state=42)\n",
        "ripper_clf.fit(train, class_feat='Category', pos_class=5)\n",
        "ripper_clf.score(X_test, y_test) # Default metric is accuracy"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9548022598870056"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IuqK1_K2A1Xn",
        "outputId": "736f4525-90b9-4294-b53f-c26c858e024f"
      },
      "source": [
        "from sklearn.metrics import precision_score, recall_score, f1_score, confusion_matrix\n",
        "precision = ripper_clf.score(X_test, y_test, precision_score)\n",
        "recall = ripper_clf.score(X_test, y_test, recall_score)\n",
        "f1_score= ripper_clf.score(X_test, y_test, f1_score)\n",
        "confusion_matrix= ripper_clf.score(X_test, y_test, confusion_matrix)\n",
        "print(f'f1_score: {f1_score}')\n",
        "print(f'precision: {precision}')\n",
        "print(f'recall: {recall}')\n",
        "print(f'confusion_matrix: {confusion_matrix}')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "f1_score: 0.42857142857142855\n",
            "precision: 0.5\n",
            "recall: 0.375\n",
            "confusion_matrix: [[166   3]\n",
            " [  5   3]]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}